# ğŸ—‚ï¸ SEEU Scraper

O **SEEU Scraper** Ã© um coletor de dados desenvolvido com **Scrapy**, projetado para extrair informaÃ§Ãµes do portal de documentaÃ§Ã£o do Sistema EletrÃ´nico de ExecuÃ§Ã£o Unificado (SEEU). Ele organiza os dados coletados em um formato estruturado, facilitando o uso posterior para anÃ¡lise ou integraÃ§Ã£o com outros sistemas.

---

## ğŸš€ Como Funciona

### **Fluxo de ExecuÃ§Ã£o**
1. **InicializaÃ§Ã£o do Spider**:
   - O spider principal (`seeu_docs`) Ã© iniciado a partir da URL base: `https://docs.seeu.pje.jus.br/docs/category/guias-de-uso-para-o-seeu/`.
   - Ele segue os links internos para explorar as pÃ¡ginas relacionadas.

2. **ExtraÃ§Ã£o de Dados**:
   - Para cada pÃ¡gina visitada, o spider coleta os seguintes campos:
     - **cluster_name**: Nome do cluster ao qual o documento pertence.
     - **title**: TÃ­tulo da pÃ¡gina ou documento.
     - **content**: ConteÃºdo textual extraÃ­do dos parÃ¡grafos da pÃ¡gina.
     - **url**: URL da pÃ¡gina de origem.

3. **Armazenamento**:
   - Os dados extraÃ­dos sÃ£o salvos no arquivo `data/seeu_docs.json` no formato JSON.
   - O arquivo Ã© sobrescrito a cada execuÃ§Ã£o para garantir que os dados estejam atualizados.

4. **ConfiguraÃ§Ã£o Personalizada**:
   - O spider utiliza a configuraÃ§Ã£o `FEED_EXPORT_FIELDS` para garantir a ordem dos campos no arquivo JSON exportado.

---

## ğŸ—ï¸ Estrutura do Projeto

```
seeu_scraper/
â”œâ”€â”€ scrapy.cfg                # ConfiguraÃ§Ã£o do Scrapy
â”œâ”€â”€ data/
â”‚   â””â”€â”€ seeu_docs.json        # Dados extraÃ­dos
â”œâ”€â”€ seeu_scraper/
â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â””â”€â”€ seeu_docs.py      # Spider principal
â”‚   â”œâ”€â”€ settings.py           # ConfiguraÃ§Ãµes do Scrapy
â”‚   â”œâ”€â”€ pipelines.py          # Processamento de itens (opcional)
â”‚   â””â”€â”€ ...                   # Outros arquivos do projeto
```

---

## ğŸ“‹ Como Executar

### **PrÃ©-requisitos**
- Python 3.12+
- DependÃªncias listadas no arquivo `requirements.txt`

### **Passos**
1. **Instale as dependÃªncias**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Execute o spider**:
   ```bash
   scrapy crawl seeu_docs
   ```

3. **Verifique os dados extraÃ­dos**:
   - Os dados estarÃ£o disponÃ­veis no arquivo `data/seeu_docs.json`.

---

## ğŸ”§ ConfiguraÃ§Ãµes Importantes

- **`DOWNLOAD_DELAY`**: Define um atraso entre as requisiÃ§Ãµes para evitar sobrecarregar o servidor.
- **`CONCURRENT_REQUESTS_PER_DOMAIN`**: Limita o nÃºmero de requisiÃ§Ãµes simultÃ¢neas para o mesmo domÃ­nio.
- **`ROBOTSTXT_OBEY`**: Configurado como `True` para respeitar as regras do arquivo `robots.txt` do site.

---

## ğŸ“‚ SaÃ­da

Os dados extraÃ­dos sÃ£o organizados no seguinte formato:

```json
[
  {
    "cluster_name": "Documentos de suporte SEEU",
    "title": "Guias de uso para o SEEU",
    "content": "Este espaÃ§o apresenta guias de uso simples e objetivos...",
    "url": "https://docs.seeu.pje.jus.br/docs/category/guias-de-uso-para-o-seeu/"
  },
  {
    "cluster_name": "Documentos de suporte SEEU",
    "title": "Portal de DocumentaÃ§Ã£o do SEEU",
    "content": "Em sintonia com as constantes melhorias do SEEU...",
    "url": "https://docs.seeu.pje.jus.br/docs/intro/"
  }
]
```

---

## ğŸ› ï¸ PersonalizaÃ§Ã£o

Para modificar o comportamento do scraper, edite os seguintes arquivos:
- **`seeu_scraper/spiders/seeu_docs.py`**: Para alterar a lÃ³gica de extraÃ§Ã£o de dados.
- **`seeu_scraper/settings.py`**: Para ajustar configuraÃ§Ãµes como delays, headers, e middlewares.

---

## ğŸ“ Suporte

Para dÃºvidas ou problemas, entre em contato com o desenvolvedor ou consulte a documentaÃ§Ã£o oficial do Scrapy:
- [DocumentaÃ§Ã£o do Scrapy](https://docs.scrapy.org/)