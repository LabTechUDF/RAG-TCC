# Guia R√°pido: M√©tricas & Qualidade

Este documento resume como usar o pacote completo de valida√ß√£o, testes, benchmarks, avalia√ß√£o e monitoramento do RAG Jur√≠dico.

## üéØ Quick Start

```bash
# 1. Instalar depend√™ncias (inclui pytest-benchmark)
poetry install --with dev

# 2. Executar workflow completo de qualidade
make quality
```

## üìã Checklist de Qualidade

### Antes de Indexar Dados

- [ ] **Validar dados**: `make data-validate`
  - Verifica campos obrigat√≥rios
  - Detecta textos curtos
  - Encontra tokens HTML residuais
  - Identifica IDs duplicados

### Durante Desenvolvimento

- [ ] **Executar testes**: `poetry run pytest -v`
  - Testes unit√°rios e funcionais
  - Skip autom√°tico de OpenSearch se n√£o dispon√≠vel
  
- [ ] **Medir performance**: `make bench`
  - Lat√™ncia de queries
  - Tempo de build de √≠ndice
  - Throughput

### Antes de Deploy

- [ ] **Avaliar recupera√ß√£o**: `make eval`
  - Precision@K, Recall@K
  - MRR, nDCG@K
  - Valida thresholds

- [ ] **Inspecionar embeddings**: `make inspect-emb`
  - Detecta NaN/Inf
  - Verifica colapso
  - Encontra duplicatas densas

## üîß Comandos Principais

### Valida√ß√£o de Dados

```bash
# B√°sico
make data-validate

# Com par√¢metros customizados
poetry run python -m src.tools.validate_data \
  --input data/merged_clean.jsonl \
  --min-chars 200 \
  --max-bad-pct 10 \
  --report reports/validation/report.json
```

**Exit codes:**
- `0`: OK, dados aprovados
- `1`: Erro de IO
- `2`: Falha por threshold

### Testes

```bash
# Todos os testes
poetry run pytest -v

# Sem OpenSearch
poetry run pytest -m "not opensearch" -v

# Com cobertura
poetry run pytest --cov=src --cov-report=html

# Apenas benchmarks
poetry run pytest tests/bench -v
```

### Benchmarks

```bash
# Executar e salvar baseline
make bench

# Comparar com baseline anterior
make bench-compare

# Ver hist√≥rico
ls .benchmarks/
```

**M√©tricas:**
- Lat√™ncia p95 de queries (SLO: 200ms)
- Tempo de build de √≠ndice (SLO: 60s)
- Throughput (m√≠n: 10 QPS)

### Avalia√ß√£o de Recupera√ß√£o

```bash
# FAISS
make eval

# OpenSearch
make eval-opensearch

# Custom
poetry run python -m src.eval.retrieval_eval \
  --qa data/eval/qa_dev.jsonl \
  --k 10 \
  --backend faiss \
  --min-p 0.6 \
  --min-ndcg 0.75
```

**M√©tricas:**
- **Precision@K**: % relevantes nos top-K
- **Recall@K**: % relevantes recuperados
- **MRR**: Posi√ß√£o do 1¬∫ relevante
- **nDCG@K**: Qualidade da ordena√ß√£o

### Inspe√ß√£o de Embeddings

```bash
# Gera embeddings on-the-fly
make inspect-emb

# De arquivo .npy
poetry run python -m src.eval.inspect_embeddings \
  --input embeddings.npy \
  --mode npy

# De JSONL com vetores
poetry run python -m src.eval.inspect_embeddings \
  --input data_with_vectors.jsonl \
  --mode vectors-jsonl \
  --near-dupes-threshold 0.99
```

## ‚öôÔ∏è Configura√ß√£o (`.env`)

```bash
# Valida√ß√£o de Dados
MIN_CHARS=200                    # Tamanho m√≠nimo de texto
VALIDATION_MAX_BAD_PCT=10        # % m√°xima de docs problem√°ticos

# SLOs e Benchmarks
SLO_P95_MS=200                   # Lat√™ncia p95 m√°xima (ms)
MAX_BUILD_TIME_S=60              # Tempo m√°x de build (s)

# Avalia√ß√£o de Recupera√ß√£o
MIN_P5=0.55                      # Precision@5 m√≠nima
MIN_NDCG5=0.70                   # nDCG@5 m√≠nimo

# Inspe√ß√£o de Embeddings
NEAR_DUPES_MAX_PCT=1             # % m√°xima de near-duplicates
```

## üöÄ CI/CD

O workflow `.github/workflows/ci.yml` executa automaticamente:

### Jobs

1. **validate_data**: Valida qualidade dos dados
2. **tests**: Testes unit√°rios/funcionais com cobertura
3. **bench**: Benchmarks de performance
4. **eval**: Avalia√ß√£o de m√©tricas de recupera√ß√£o
5. **lint**: Formata√ß√£o e estilo de c√≥digo
6. **summary**: Resumo agregado

### Triggers

- Push em `main`, `develop`, `chore/consolidando-aplicacao`
- Pull requests para `main`
- Agendado: diariamente √†s 6h UTC

### Artifacts

- `validation-report`: Relat√≥rio de valida√ß√£o
- `benchmark-results`: Resultados de benchmarks
- `eval-results-*`: M√©tricas de recupera√ß√£o
- Cobertura de c√≥digo (Codecov)

## üìä Estrutura de Relat√≥rios

```
reports/
‚îú‚îÄ‚îÄ validation/
‚îÇ   ‚îî‚îÄ‚îÄ report.json              # Qualidade dos dados
‚îú‚îÄ‚îÄ eval/
‚îÇ   ‚îú‚îÄ‚îÄ retrieval_metrics.json   # M√©tricas agregadas
‚îÇ   ‚îî‚îÄ‚îÄ retrieval_metrics.csv    # Detalhes por query
‚îî‚îÄ‚îÄ inspect/
    ‚îî‚îÄ‚îÄ embeddings_summary.json  # Sa√∫de dos embeddings
```

## üéì Exemplos de Uso

### Validar Antes de Indexar

```bash
# 1. Mesclar dados
make data-merge

# 2. Validar qualidade
make data-validate

# 3. Se aprovado, indexar
make faiss-build
```

### Medir Performance Antes de Deploy

```bash
# 1. Build √≠ndice com dados reais
make faiss-build

# 2. Executar benchmarks
make bench

# 3. Verificar se atende SLOs
cat .benchmarks/*/0001_*.json | grep "mean"
```

### Avaliar Qualidade de Recupera√ß√£o

```bash
# 1. Preparar dataset Q&A
# Editar: data/eval/qa_dev.jsonl

# 2. Executar avalia√ß√£o
make eval

# 3. Verificar m√©tricas
cat reports/eval/retrieval_metrics.json

# 4. Analisar queries problem√°ticas
cat reports/eval/retrieval_metrics.csv
```

### Debugar Embeddings

```bash
# 1. Inspecionar vetores
make inspect-emb

# 2. Ver relat√≥rio
cat reports/inspect/embeddings_summary.json

# 3. Se houver problemas, verificar:
#    - NaNs/Infs (bug no modelo?)
#    - Colapso (normaliza√ß√£o?)
#    - Duplicatas (dados duplicados?)
```

## üêõ Troubleshooting

### Valida√ß√£o Falha

**Problema:** `bad_overall_pct > max-bad-pct`

**Solu√ß√µes:**
- Aumentar `--max-bad-pct` temporariamente
- Limpar dados com `src.tools.tratamento_dados`
- Filtrar registros problem√°ticos

### Benchmarks Abaixo do SLO

**Problema:** Lat√™ncia p95 > 200ms

**Solu√ß√µes:**
- Verificar tamanho do √≠ndice
- Considerar quantiza√ß√£o (FAISS)
- Mover para OpenSearch (cache)

### M√©tricas de Recupera√ß√£o Baixas

**Problema:** nDCG@5 < 0.70

**Solu√ß√µes:**
- Revisar dataset Q&A (ground-truth correto?)
- Testar diferentes modelos de embedding
- Considerar busca h√≠brida (BM25 + kNN)

### Embeddings com NaN

**Problema:** `has_nan: true`

**Solu√ß√µes:**
- Verificar textos vazios/inv√°lidos
- Atualizar sentence-transformers
- Validar dados de entrada

## üìö Refer√™ncias

- **pytest-benchmark**: https://pytest-benchmark.readthedocs.io/
- **M√©tricas de IR**: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)
- **FAISS**: https://github.com/facebookresearch/faiss
- **OpenSearch**: https://opensearch.org/docs/latest/

## ü§ù Contribuindo

Ao adicionar features, sempre inclua:

1. Testes unit√°rios (`tests/test_*.py`)
2. Benchmarks se aplic√°vel (`tests/bench/`)
3. Documenta√ß√£o no README
4. Atualiza√ß√£o dos thresholds (`.env.example`)

---

**üí° Dica:** Execute `make quality` antes de cada commit para garantir que tudo est√° funcionando corretamente!
