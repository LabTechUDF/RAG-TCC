# ============================================================================
# CONFIGURAÇÃO OTIMIZADA - RAG TCC
# ============================================================================
# Baseado em análise de performance da indexação de 724k documentos
# Throughput observado: ~72-90 docs/s, ~3 batches/s de embedding
# ============================================================================

# Backend de busca vetorial (faiss|opensearch)
SEARCH_BACKEND=faiss

# ----------------------------------------------------------------------------
# EMBEDDINGS
# ----------------------------------------------------------------------------
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIM=384
NORMALIZE_EMBEDDINGS=true

# Batch size para geração de embeddings (32 textos por batch interno do modelo)
# Valor ótimo observado: 1000 docs = 32 batches de 32 textos cada (~10s)
EMBEDDING_BATCH_SIZE=32

# ----------------------------------------------------------------------------
# FAISS - INDEXAÇÃO
# ----------------------------------------------------------------------------
FAISS_INDEX_PATH=data/indexes/faiss
FAISS_METADATA_PATH=data/indexes/faiss/metadata.parquet

# OTIMIZAÇÕES DE BUILD:
# - batch_size: documentos por lote de embedding (1000 = ~10s de processamento)
# - buffer_batches: quantos batches acumular antes de adicionar ao índice (10 = 10k docs)
# - save_every: salvar checkpoint a cada N documentos (50k = ~9 min de indexação)
FAISS_BUILD_BATCH_SIZE=1000
FAISS_BUFFER_BATCHES=10
FAISS_SAVE_EVERY=50000

# GPU (requer ambiente Conda GPU e FAISS com suporte CUDA)
USE_FAISS_GPU=false
FAISS_GPU_DEVICE=0

# ----------------------------------------------------------------------------
# OPENSEARCH (alternativo ao FAISS)
# ----------------------------------------------------------------------------
OPENSEARCH_HOST=localhost
OPENSEARCH_PORT=9200
OPENSEARCH_INDEX=juridico-docs
OPENSEARCH_USERNAME=
OPENSEARCH_PASSWORD=
OPENSEARCH_USE_SSL=false

# ----------------------------------------------------------------------------
# API
# ----------------------------------------------------------------------------
API_HOST=0.0.0.0
API_PORT=8000

# Query padrão para testes
QUERY=direitos fundamentais

# ----------------------------------------------------------------------------
# LLM PARA RAG (NOVO)
# ----------------------------------------------------------------------------
# Provider: openai ou anthropic
LLM_PROVIDER=openai

# Chaves de API - configure pelo menos uma
OPENAI_API_KEY=sk-your-key-here
# ANTHROPIC_API_KEY=sk-ant-your-key-here

# Modelos recomendados:
# OpenAI: gpt-4o-mini (rápido), gpt-4o (melhor qualidade)
# Anthropic: claude-3-haiku-20240307 (rápido), claude-3-5-sonnet-20241022 (melhor)
OPENAI_MODEL=gpt-4o-mini
ANTHROPIC_MODEL=claude-3-haiku-20240307

# ----------------------------------------------------------------------------
# VALIDAÇÃO E QUALIDADE
# ----------------------------------------------------------------------------
# Comprimento mínimo de documento (chars)
MIN_CHARS=200

# Porcentagem máxima de documentos "ruins" permitida
VALIDATION_MAX_BAD_PCT=10

# SLO de latência: P95 < 200ms (observado: 45ms avg)
SLO_P95_MS=200

# Tempo máximo de build (para testes de performance)
MAX_BUILD_TIME_S=60

# ----------------------------------------------------------------------------
# AVALIAÇÃO DE RECUPERAÇÃO
# ----------------------------------------------------------------------------
# Thresholds mínimos de qualidade para retrieval
MIN_P5=0.55      # Precision@5 mínima
MIN_NDCG5=0.70   # NDCG@5 mínimo

# ----------------------------------------------------------------------------
# INSPEÇÃO DE EMBEDDINGS
# ----------------------------------------------------------------------------
# Porcentagem máxima de duplicados próximos (near-duplicates)
NEAR_DUPES_MAX_PCT=1

# ----------------------------------------------------------------------------
# LOGGING E DEBUG
# ----------------------------------------------------------------------------
LOG_LEVEL=INFO