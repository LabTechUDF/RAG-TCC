"""
Servi√ßo RAG Orquestrador para consultas jur√≠dicas SEEU.
Coordena: normaliza√ß√£o ‚Üí busca vetorial ‚Üí LLM ‚Üí resposta estruturada.
"""
import os
import json
import logging
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

from openai import OpenAI
from anthropic import Anthropic

from src.rag_schemas import (
    RagQueryRequest,
    RagQueryResponse,
    QueryNormalizadaOutput,
    ChunkWithScore,
    ChunkMetadata,
    TeseJuridica,
    JurisprudenciaReferencia
)
from src.rag_normalizer import get_normalizer
from src.storage.base import VectorStore
from src.schema import SearchResult
from src import embeddings
from src.request_logger import RequestLogger

log = logging.getLogger(__name__)


# ========================================
# TEMPLATE DO PROMPT FINAL SEEU
# ========================================

TEMPLATE_RAG_SEEU = """Voc√™ √© um assistente jur√≠dico especializado em **execu√ß√£o penal** e no sistema **SEEU** (Sistema Eletr√¥nico de Execu√ß√£o Unificado).

**CONTEXTO DA CONSULTA:**
Query original do usu√°rio: "{query_original}"
Query normalizada (t√©cnica): "{query_normalizada}"

**DADOS DE EXECU√á√ÉO PENAL IDENTIFICADOS:**
{dados_execucao}

**TEMAS RELACIONADOS:**
{temas_execucao}

**PALAVRAS-CHAVE JUR√çDICAS:**
{palavras_chave}

**DOCUMENTOS JURISPRUDENCIAIS RECUPERADOS:**

{documentos_contexto}

---

**SUA TAREFA:**
Com base EXCLUSIVAMENTE nos documentos acima e nos dados de execu√ß√£o penal fornecidos, elabore uma an√°lise jur√≠dica estruturada.

**ESTRUTURA DA RESPOSTA (JSON):**

{{
  "contexto_seeu": "<Explique brevemente o contexto da execu√ß√£o penal e como o SEEU se relaciona com o caso>",
  
  "teses": [
    {{
      "titulo": "<T√≠tulo da tese jur√≠dica>",
      "descricao": "<Explica√ß√£o detalhada da tese com base na jurisprud√™ncia>",
      "documentosSuporte": [<lista de IDs dos documentos que sustentam esta tese>]
    }}
  ],
  
  "aplicacao_caso": "<Aplica√ß√£o pr√°tica ao caso concreto, considerando os dados de execu√ß√£o penal fornecidos>",
  
  "jurisprudencias": [
    {{
      "docId": <ID do documento>,
      "tribunal": "<Tribunal>",
      "processo": "<N√∫mero do processo>",
      "ano": <Ano da decis√£o>,
      "tema": "<Tema principal>",
      "relevanciaRelativa": <Relev√¢ncia em %>,
      "trechoUtilizado": "<Trecho espec√≠fico que fundamenta a an√°lise>"
    }}
  ],
  
  "avisos_limitacoes": "<Avisos sobre limita√ß√µes da an√°lise e car√°ter meramente informativo>"
}}

**REGRAS CR√çTICAS:**
1. Use APENAS informa√ß√µes presentes nos documentos fornecidos
2. N√ÉO invente n√∫meros de processo, datas ou fatos
3. Para cada tese, cite os documentos que a sustentam (use os IDs: Documento 1, 2, 3...)
4. Na se√ß√£o "jurisprudencias", inclua TODOS os documentos relevantes com seus trechos
5. A relev√¢ncia relativa j√° est√° calculada - use o valor fornecido
6. Se faltarem informa√ß√µes de execu√ß√£o penal, mencione isso em "avisos_limitacoes"
7. Mantenha linguagem t√©cnico-jur√≠dica mas compreens√≠vel

Retorne apenas o JSON (sem markdown):"""


# ========================================
# TEMPLATE MARKDOWN PARA UX JUR√çDICA SEEU
# ========================================

TEMPLATE_RAG_SEEU_MARKDOWN = """Voc√™ √© um assistente jur√≠dico especializado em **execu√ß√£o penal** e no sistema **SEEU**.

**CONTEXTO DA CONSULTA:**
- Query original: "{query_original}"
- Query normalizada: "{query_normalizada}"

{historico_conversa}

**DADOS DE EXECU√á√ÉO PENAL IDENTIFICADOS:**
{dados_execucao}

**TEMAS RELACIONADOS:** {temas_execucao}

**PALAVRAS-CHAVE JUR√çDICAS:** {palavras_chave}

**DOCUMENTOS JURISPRUDENCIAIS RECUPERADOS:**

{documentos_contexto}

---

**SUA TAREFA:**
Gerar uma resposta em Markdown LIMPO e BEM FORMATADO. Siga as diretrizes abaixo de forma FLEX√çVEL - adapte as se√ß√µes conforme necess√°rio para responder da melhor forma poss√≠vel.

**REGRAS DE FORMATA√á√ÉO:**
1. Use quebras de linha duplas entre se√ß√µes
2. Use bullets (-) para listas
3. Use **negrito** para destacar informa√ß√µes importantes
4. Mantenha par√°grafos curtos e diretos

**REGRAS CR√çTICAS - LEIA COM ATEN√á√ÉO:**

1. **INCLUA TODAS as informa√ß√µes dispon√≠veis nos documentos.** Cada documento fornecido cont√©m metadados como Tribunal, N√∫mero do Processo, Relator, Data de Julgamento, √ìrg√£o Julgador, Tema. **USE ESSES DADOS** na sua resposta - eles s√£o importantes para a fundamenta√ß√£o jur√≠dica.

2. **NUNCA use placeholders gen√©ricos.** N√£o escreva "[TRIBUNAL]", "[N√öMERO]", "[ANO]", "XX.X%". Se um campo espec√≠fico n√£o est√° dispon√≠vel no documento, simplesmente n√£o mencione esse campo - mas INCLUA todos os campos que EST√ÉO dispon√≠veis.

3. **Baseie-se APENAS nos documentos fornecidos.** N√ÉO invente n√∫meros de processos, tribunais, datas ou URLs. Use EXATAMENTE os dados fornecidos nos documentos acima.

4. **Se a informa√ß√£o for insuficiente para responder, FA√áA PERGUNTAS.** Ao inv√©s de dar uma resposta incompleta ou gen√©rica, pergunte ao usu√°rio o que voc√™ precisa saber para ajud√°-lo melhor. Exemplos:
   - "Para analisar melhor seu caso, preciso saber: qual √© o regime atual do apenado?"
   - "Voc√™ poderia informar h√° quanto tempo a pena est√° sendo cumprida?"
   - "O crime cometido √© hediondo ou comum?"

5. **Se os documentos n√£o tratam do tema perguntado, seja transparente.** Diga claramente que a base de dados n√£o cont√©m jurisprud√™ncia espec√≠fica sobre aquele tema.

6. **Considere o hist√≥rico da conversa** para dar respostas contextualizadas e evitar repetir informa√ß√µes j√° fornecidas.

7. **Seja CONCISO e DIRETO.** N√£o repita se√ß√µes vazias.

8. **NUNCA exiba "N/A" ou campos vazios.** Se um campo n√£o est√° dispon√≠vel, simplesmente omita-o.

9. **Diferencie tipos de documentos:**
   - A se√ß√£o "üìö Documentos Analisados" deve listar TODOS os documentos relevantes (leis, jurisprud√™ncia, doutrina, etc.) com as informa√ß√µes dispon√≠veis.
   - A se√ß√£o "‚öñÔ∏è Jurisprud√™ncias Relevantes" s√≥ deve aparecer se houver documentos de JURISPRUD√äNCIA com metadados completos (n√∫mero do processo, relator, ano). Se os documentos forem apenas leis ou n√£o tiverem esses metadados, **OMITA completamente esta se√ß√£o**.

---

**ESTRUTURA DA RESPOSTA (siga fielmente):**

## üìã Resumo Objetivo

- [Bullet 1: Resposta direta √† pergunta do usu√°rio]
- [Bullet 2: Principais conclus√µes baseadas nos documentos]
- [Bullet 3: Limita√ß√µes ou observa√ß√µes importantes]

---

## üìö Documentos Analisados

Para CADA documento relevante, adapte o formato conforme o tipo de documento:

**Para Jurisprud√™ncia:**
**Documento 1 ‚Äì [TRIBUNAL] ‚Äì [PROCESSO] ‚Äì [ANO]**
- **Relev√¢ncia:** [percentual]%
- **Tema:** [tema central]

**Para Legisla√ß√£o:**
**Documento X ‚Äì [Nome da Lei/C√≥digo] ‚Äì [Artigo]**
- **Relev√¢ncia:** [percentual]%
- **Conte√∫do:** [resumo do que o artigo disp√µe]

**Para outros documentos (doutrina, s√∫mulas, etc.):**
**Documento X ‚Äì [Tipo] ‚Äì [Fonte/Autor se dispon√≠vel]**
- **Relev√¢ncia:** [percentual]%
- **Tema:** [tema central]

[Inclua apenas os campos que existem no documento - n√£o use "N/A"]

---

## ‚öñÔ∏è Jurisprud√™ncias Relevantes

**‚ö†Ô∏è IMPORTANTE: S√≥ inclua esta se√ß√£o se houver documentos de JURISPRUD√äNCIA com metadados reais (processo, relator, ano). Caso contr√°rio, OMITA esta se√ß√£o completamente.**

Para CADA jurisprud√™ncia com dados completos:

### üìå [TRIBUNAL] ‚Äì Processo n¬∫ [N√öMERO COMPLETO DO PROCESSO]

**üìä Relev√¢ncia:** [percentual]%
**üìÖ Ano:** [ano]
**üèõÔ∏è Relator(a):** [nome do relator]
**üìë Tema:** [tema principal]

**üí° Trecho Relevante:**
> "[Copie o trecho mais importante do documento que fundamenta a an√°lise]"

---

[Repita para cada jurisprud√™ncia - mas s√≥ se tiver os dados acima]

---

## ‚úÖ Conclus√£o

- [Bullet 1: √â poss√≠vel ou n√£o responder √† pergunta com base nos documentos?]
- [Bullet 2: O que FALTA de informa√ß√£o, se for o caso]
- [Bullet 3: Leitura mais prudente diante da jurisprud√™ncia encontrada]
- [Bullet 4: Recomenda√ß√µes pr√°ticas para o caso]

---

## üéØ Pr√≥ximos Passos Sugeridos

1. **Legisla√ß√£o:**
   - Consultar [artigos espec√≠ficos relevantes ao caso]

2. **Pesquisa Complementar:**
   - Buscar jurisprud√™ncia sobre [temas relacionados]

3. **Dados do Caso:**
   - Obter informa√ß√£o sobre [dados que ajudariam na an√°lise]

4. **Documenta√ß√£o:**
   - Reunir documentos comprobat√≥rios de [requisitos espec√≠ficos]

---

## ‚ö†Ô∏è Avisos e Limita√ß√µes

- ‚úì Esta resposta tem car√°ter **informativo e consultivo**
- ‚úì **N√ÉO substitui** an√°lise t√©cnico-jur√≠dica completa do processo
- ‚úì Baseada **exclusivamente** nos documentos retornados pelo sistema
- ‚úì Recomenda-se consulta aos autos originais e verifica√ß√£o de jurisprud√™ncia mais recente

---

Retorne APENAS o texto em Markdown (sem c√≥digo markdown com ```):"""


# ========================================
# FUN√á√ïES AUXILIARES
# ========================================

def calcular_relevancia_relativa(scores: List[float]) -> List[float]:
    """
    Calcula relev√¢ncia em porcentagem diretamente do score.
    
    Args:
        scores: Lista de scores brutos de similaridade (0 a 1)
        
    Returns:
        Lista de relev√¢ncias em porcentagem (score * 100)
    """
    if not scores:
        return []
    
    # Converte score diretamente para porcentagem
    relevancia_relativa = [score * 100 for score in scores]
    
    return relevancia_relativa


def agrupar_chunks_por_documento(
    chunks: List[ChunkWithScore]
) -> Dict[str, List[ChunkWithScore]]:
    """
    Agrupa chunks pelo ID do documento global.
    
    Returns:
        Dict com chave = idDocumentoGlobal, valor = lista de chunks
    """
    docs: Dict[str, List[ChunkWithScore]] = {}
    
    for chunk in chunks:
        doc_id = chunk.metadata.idDocumentoGlobal
        if doc_id not in docs:
            docs[doc_id] = []
        docs[doc_id].append(chunk)
    
    return docs


def montar_contexto_documentos(
    chunks_agrupados: Dict[str, List[ChunkWithScore]]
) -> str:
    """
    Monta string de contexto formatado para o LLM.
    
    Cada documento √© numerado sequencialmente com metadados completos.
    """
    contexto_partes = []
    doc_numero = 1
    
    for doc_id, chunks in chunks_agrupados.items():
        # Ordena chunks por posi√ß√£o se dispon√≠vel
        chunks_ordenados = sorted(
            chunks,
            key=lambda c: c.metadata.posicaoChunk or 0
        )
        
        # Pega metadados do primeiro chunk
        meta = chunks_ordenados[0].metadata
        
        # Cabe√ßalho do documento com formata√ß√£o melhorada
        contexto_partes.append(f"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        contexto_partes.append(f"üìÑ DOCUMENTO {doc_numero}")
        contexto_partes.append(f"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        
        if meta.tribunal:
            contexto_partes.append(f"üèõÔ∏è  Tribunal: {meta.tribunal}")
        if meta.numeroProcesso:
            contexto_partes.append(f"üìã Processo: {meta.numeroProcesso}")
        if meta.relator:
            contexto_partes.append(f"üë§ Relator(a): {meta.relator}")
        if meta.dataJulgamento:
            contexto_partes.append(f"üìÖ Data Julgamento: {meta.dataJulgamento}")
        if meta.orgaoJulgador:
            contexto_partes.append(f"‚öñÔ∏è  √ìrg√£o Julgador: {meta.orgaoJulgador}")
        if meta.tema:
            contexto_partes.append(f"üîñ Tema: {meta.tema}")
        
        contexto_partes.append("")
        contexto_partes.append("üìù TRECHOS RELEVANTES:")
        contexto_partes.append("")
        
        # Chunks do documento
        for i, chunk in enumerate(chunks_ordenados, 1):
            relevancia = chunk.relevanciaRelativa or (chunk.score * 100)
            contexto_partes.append(f"‚ñ∏ Trecho {i} (Relev√¢ncia: {relevancia:.1f}%):")
            contexto_partes.append(f'  "{chunk.texto}"')
            contexto_partes.append("")
        
        doc_numero += 1
    
    return "\n".join(contexto_partes)


# ========================================
# SERVI√áO RAG
# ========================================

class RagService:
    """Servi√ßo orquestrador de RAG jur√≠dico."""
    
    def __init__(
        self,
        store: VectorStore,
        provider: str = "openai",
        model: Optional[str] = None,
        api_key: Optional[str] = None
    ):
        """
        Inicializa o servi√ßo RAG.
        
        Args:
            store: Store vetorial (FAISS, OpenSearch, etc.)
            provider: Provedor do LLM ("openai" ou "anthropic")
            model: Nome do modelo
            api_key: Chave da API
        """
        self.store = store
        self.provider = provider.lower()
        
        # Configura√ß√£o do LLM
        if self.provider == "openai":
            self.model = model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            api_key = api_key or os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY n√£o configurada")
            self.client = OpenAI(api_key=api_key)
            
        elif self.provider == "anthropic":
            self.model = model or os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet-20241022")
            api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEY n√£o configurada")
            self.client = Anthropic(api_key=api_key)
        else:
            raise ValueError(f"Provider n√£o suportado: {provider}")
        
        log.info(f"RagService inicializado: {self.provider} / {self.model}")
    
    def processar_consulta(
        self,
        request: RagQueryRequest
    ) -> RagQueryResponse:
        """
        Processa consulta RAG completa.
        
        Fluxo:
        1. Normaliza√ß√£o jur√≠dica
        2. Busca vetorial
        3. C√°lculo de relev√¢ncia relativa
        4. Constru√ß√£o do prompt
        5. Chamada ao LLM
        6. Parse e estrutura√ß√£o da resposta
        
        Args:
            request: Request com prompt do usu√°rio e metadados
            
        Returns:
            RagQueryResponse estruturada
        """
        log.info(f"Processando consulta RAG: {request.promptUsuario[:100]}...")
        
        # ETAPA 1: Normaliza√ß√£o Jur√≠dica
        normalizer = get_normalizer()
        contexto_meta = self._formatar_contexto_metadados(request.metadados)
        query_normalizada = normalizer.normalizar(
            prompt_usuario=request.promptUsuario,
            contexto_adicional=contexto_meta
        )
        
        log.info(f"Query normalizada: {query_normalizada.queryRAG}")
        
        # ETAPA 2: Busca Vetorial
        if not request.useRag:
            # Modo sem RAG - retorna resposta direta (TODO: implementar)
            return self._resposta_sem_rag(request, query_normalizada)
        
        chunks_recuperados = self._buscar_chunks(
            query_normalizada.queryRAG,
            k=request.k,
            metadados=request.metadados
        )
        
        if not chunks_recuperados:
            log.warning("Nenhum chunk recuperado")
            return self._resposta_vazia(request, query_normalizada)
        
        # ETAPA 3: Calcular Relev√¢ncia Relativa
        scores = [c.score for c in chunks_recuperados]
        relevancia_relativa = calcular_relevancia_relativa(scores)
        
        for i, chunk in enumerate(chunks_recuperados):
            chunk.relevanciaRelativa = round(relevancia_relativa[i], 1)
        
        # ETAPA 4: Agrupar por Documento
        chunks_agrupados = agrupar_chunks_por_documento(chunks_recuperados)
        
        log.info(
            f"Recuperados {len(chunks_recuperados)} chunks de "
            f"{len(chunks_agrupados)} documentos √∫nicos"
        )
        
        # ETAPA 5: Montar Prompt e Chamar LLM
        resposta_llm = self._gerar_resposta_llm(
            request.promptUsuario,
            query_normalizada,
            chunks_agrupados
        )
        
        # ETAPA 6: Estruturar Resposta Final
        resposta_final = self._estruturar_resposta(
            request,
            query_normalizada,
            chunks_recuperados,
            chunks_agrupados,
            resposta_llm
        )
        
        log.info("Consulta RAG processada com sucesso")
        return resposta_final
    
    def _formatar_contexto_metadados(self, metadados) -> str:
        """Formata metadados para contexto do normalizador."""
        if not metadados:
            return "Nenhum metadado adicional."
        
        partes = []
        if metadados.tribunal:
            partes.append(f"Tribunal: {metadados.tribunal}")
        if metadados.anoMin or metadados.anoMax:
            partes.append(f"Per√≠odo: {metadados.anoMin or '?'} - {metadados.anoMax or '?'}")
        if metadados.tipoConsulta:
            partes.append(f"Tipo: {metadados.tipoConsulta}")
        
        return " | ".join(partes) if partes else "Nenhum metadado adicional."
    
    def _buscar_chunks(
        self,
        query: str,
        k: int,
        metadados
    ) -> List[ChunkWithScore]:
        """
        Executa busca vetorial e converte para ChunkWithScore.
        
        TODO: Implementar filtros de metadados quando store suportar.
        """
        # Gera embedding
        query_vector = embeddings.encode_single_text(query)
        
        # Busca no store
        resultados: List[SearchResult] = self.store.search(query_vector, k=k)
        
        # Converte para ChunkWithScore
        chunks = []
        for resultado in resultados:
            doc = resultado.doc
            
            # Extrai metadados do chunk
            meta_dict = doc.meta or {}
            metadata = ChunkMetadata(
                idDocumentoGlobal=meta_dict.get("idDocumentoGlobal", doc.id),
                idChunk=doc.id,
                tribunal=doc.court or meta_dict.get("tribunal"),
                numeroProcesso=meta_dict.get("numeroProcesso"),
                orgaoJulgador=meta_dict.get("orgaoJulgador"),
                relator=meta_dict.get("relator"),
                dataJulgamento=doc.date or meta_dict.get("dataJulgamento"),
                tema=meta_dict.get("tema"),
                fonte=meta_dict.get("fonte"),
                posicaoChunk=meta_dict.get("posicaoChunk"),
                totalChunks=meta_dict.get("totalChunks")
            )
            
            chunk = ChunkWithScore(
                texto=doc.text,
                metadata=metadata,
                score=resultado.score
            )
            chunks.append(chunk)
        
        return chunks
    
    def _gerar_resposta_llm(
        self,
        query_original: str,
        query_normalizada: QueryNormalizadaOutput,
        chunks_agrupados: Dict[str, List[ChunkWithScore]]
    ) -> Dict[str, Any]:
        """Gera resposta estruturada usando LLM."""
        
        # Formata dados de execu√ß√£o penal
        dados_exec = query_normalizada.dadosExecucaoPenal
        dados_exec_str = json.dumps(dados_exec.dict(), ensure_ascii=False, indent=2)
        
        # Formata temas e palavras-chave
        temas_str = ", ".join(query_normalizada.temaExecucao) if query_normalizada.temaExecucao else "Nenhum tema espec√≠fico identificado"
        palavras_str = ", ".join(query_normalizada.palavrasChaveJuridicas) if query_normalizada.palavrasChaveJuridicas else "Nenhuma palavra-chave espec√≠fica"
        
        # Monta contexto dos documentos
        contexto_docs = montar_contexto_documentos(chunks_agrupados)
        
        # Monta prompt final
        prompt = TEMPLATE_RAG_SEEU.format(
            query_original=query_original,
            query_normalizada=query_normalizada.queryRAG,
            dados_execucao=dados_exec_str,
            temas_execucao=temas_str,
            palavras_chave=palavras_str,
            documentos_contexto=contexto_docs
        )
        
        log.debug(f"Prompt final montado ({len(prompt)} chars)")
        
        # Chama LLM
        resposta_raw = self._chamar_llm(prompt)
        
        # Parse JSON
        return self._parse_resposta_llm(resposta_raw)
    
    def _chamar_llm(self, prompt: str) -> str:
        """Chama o LLM apropriado."""
        if self.provider == "openai":
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Voc√™ √© um assistente jur√≠dico especializado em execu√ß√£o penal. Retorne apenas JSON v√°lido."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.4,
                max_tokens=4000
            )
            return response.choices[0].message.content.strip()
        
        elif self.provider == "anthropic":
            response = self.client.messages.create(
                model=self.model,
                max_tokens=4000,
                temperature=0.4,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return response.content[0].text.strip()
        
        raise ValueError(f"Provider n√£o suportado: {self.provider}")
    
    def _parse_resposta_llm(self, resposta_raw: str) -> Dict[str, Any]:
        """Parse da resposta JSON do LLM."""
        # Remove markdown fences
        resposta_limpa = resposta_raw.strip()
        if resposta_limpa.startswith("```json"):
            resposta_limpa = resposta_limpa[7:]
        if resposta_limpa.startswith("```"):
            resposta_limpa = resposta_limpa[3:]
        if resposta_limpa.endswith("```"):
            resposta_limpa = resposta_limpa[:-3]
        resposta_limpa = resposta_limpa.strip()
        
        try:
            return json.loads(resposta_limpa)
        except json.JSONDecodeError as e:
            log.error(f"Erro ao fazer parse do JSON do LLM: {e}")
            log.error(f"Resposta recebida: {resposta_raw[:500]}")
            raise ValueError(f"LLM retornou JSON inv√°lido: {e}")
    
    def _estruturar_resposta(
        self,
        request: RagQueryRequest,
        query_normalizada: QueryNormalizadaOutput,
        chunks_todos: List[ChunkWithScore],
        chunks_agrupados: Dict[str, List[ChunkWithScore]],
        resposta_llm: Dict[str, Any]
    ) -> RagQueryResponse:
        """Estrutura resposta final no formato RagQueryResponse."""
        
        # Extrai campos do LLM
        teses_llm = resposta_llm.get("teses", [])
        jurisp_llm = resposta_llm.get("jurisprudencias", [])
        
        # Converte para Pydantic
        teses = [TeseJuridica(**t) for t in teses_llm]
        jurisprudencias = [JurisprudenciaReferencia(**j) for j in jurisp_llm]
        
        return RagQueryResponse(
            queryOriginal=request.promptUsuario,
            queryNormalizada=query_normalizada,
            timestampConsulta=datetime.utcnow().isoformat(),
            contexto_seeu=resposta_llm.get("contexto_seeu", ""),
            teses=teses,
            aplicacao_caso=resposta_llm.get("aplicacao_caso", ""),
            jurisprudencias=jurisprudencias,
            avisos_limitacoes=resposta_llm.get("avisos_limitacoes", ""),
            backend=self.store.__class__.__name__,
            totalChunksRecuperados=len(chunks_todos),
            totalDocumentosUnicos=len(chunks_agrupados)
        )
    
    def _resposta_sem_rag(
        self,
        request: RagQueryRequest,
        query_normalizada: QueryNormalizadaOutput
    ) -> RagQueryResponse:
        """Resposta quando useRag=False (TODO: implementar)."""
        return RagQueryResponse(
            queryOriginal=request.promptUsuario,
            queryNormalizada=query_normalizada,
            contexto_seeu="Modo sem RAG - resposta direta (n√£o implementado)",
            teses=[],
            aplicacao_caso="",
            jurisprudencias=[],
            avisos_limitacoes="Funcionalidade em desenvolvimento",
            backend="none",
            totalChunksRecuperados=0,
            totalDocumentosUnicos=0
        )
    
    def _resposta_vazia(
        self,
        request: RagQueryRequest,
        query_normalizada: QueryNormalizadaOutput
    ) -> RagQueryResponse:
        """Resposta quando nenhum chunk √© recuperado."""
        return RagQueryResponse(
            queryOriginal=request.promptUsuario,
            queryNormalizada=query_normalizada,
            contexto_seeu="Nenhum documento relevante encontrado na base de dados.",
            teses=[],
            aplicacao_caso="",
            jurisprudencias=[],
            avisos_limitacoes="N√£o foram encontrados documentos jurisprudenciais relevantes para esta consulta. Considere reformular a pergunta ou verificar se a base de dados cont√©m informa√ß√µes sobre o tema.",
            backend=self.store.__class__.__name__,
            totalChunksRecuperados=0,
            totalDocumentosUnicos=0
        )

    def query_markdown(
        self,
        request: RagQueryRequest
    ) -> str:
        """
        Processa consulta RAG e retorna resposta em Markdown puro (formato UX jur√≠dica).

        Este m√©todo √© otimizado para exibi√ß√£o direta na interface do usu√°rio,
        seguindo as diretrizes de UX jur√≠dica do SEEU.

        Fluxo:
        1. Normaliza√ß√£o jur√≠dica
        2. Busca vetorial
        3. C√°lculo de relev√¢ncia relativa
        4. Constru√ß√£o do prompt Markdown
        5. Chamada ao LLM
        6. Retorno direto do Markdown gerado

        Args:
            request: Request com prompt do usu√°rio e metadados

        Returns:
            String em Markdown formatado para operadores do direito
        """
        # Inicializa logger de requisi√ß√£o para observabilidade
        req_logger = RequestLogger()

        try:
            log.info(f"Processando consulta RAG (Markdown): {request.promptUsuario[:100]}...")

            # Log da requisi√ß√£o inicial
            req_logger.log_request(
                prompt=request.promptUsuario,
                use_rag=request.useRag,
                k=request.k,
                metadados=request.metadados.dict() if request.metadados else {}
            )

            # Converte hist√≥rico para formato dict e loga
            history_dicts = [{"role": h.role, "content": h.content} for h in request.history] if request.history else []
            req_logger.log_history(history_dicts)

            # ETAPA 1: Normaliza√ß√£o Jur√≠dica
            normalizer = get_normalizer()
            contexto_meta = self._formatar_contexto_metadados(request.metadados)
            query_normalizada = normalizer.normalizar(
                prompt_usuario=request.promptUsuario,
                contexto_adicional=contexto_meta
            )

            log.info(f"Query normalizada: {query_normalizada.queryRAG}")
            req_logger.log_normalization(query_normalizada.dict())

            # ETAPA 2: Busca Vetorial
            if not request.useRag:
                response = self._resposta_markdown_sem_rag(request, query_normalizada)
                req_logger.log_final_response(response)
                req_logger.add_metadata("mode", "sem_rag")
                req_logger.save()
                return response

            chunks_recuperados = self._buscar_chunks(
                query_normalizada.queryRAG,
                k=request.k,
                metadados=request.metadados
            )

            if not chunks_recuperados:
                log.warning("Nenhum chunk recuperado")
                response = self._resposta_markdown_vazia(request, query_normalizada)
                req_logger.log_final_response(response)
                req_logger.add_metadata("no_chunks_found", True)
                req_logger.save()
                return response

            # ETAPA 3: Calcular Relev√¢ncia Relativa
            scores = [c.score for c in chunks_recuperados]
            relevancia_relativa = calcular_relevancia_relativa(scores)

            for i, chunk in enumerate(chunks_recuperados):
                chunk.relevanciaRelativa = round(relevancia_relativa[i], 1)

            # ETAPA 4: Agrupar por Documento
            chunks_agrupados = agrupar_chunks_por_documento(chunks_recuperados)

            log.info(
                f"Recuperados {len(chunks_recuperados)} chunks de "
                f"{len(chunks_agrupados)} documentos √∫nicos"
            )

            # Log dos documentos recuperados para observabilidade
            docs_for_log = []
            for doc_id, chunks in chunks_agrupados.items():
                meta = chunks[0].metadata
                docs_for_log.append({
                    "doc_id": doc_id,
                    "tribunal": meta.tribunal,
                    "processo": meta.numeroProcesso,
                    "relator": meta.relator,
                    "data": meta.dataJulgamento,
                    "tema": meta.tema,
                    "num_chunks": len(chunks),
                    "relevancia": chunks[0].relevanciaRelativa,
                    "texto_preview": chunks[0].texto[:300] + "..." if len(chunks[0].texto) > 300 else chunks[0].texto
                })
            req_logger.log_retrieved_documents(docs_for_log)

            # ETAPA 5: Gerar cabe√ßalho informativo
            num_docs = len(chunks_agrupados)
            cabecalho = f"üìö Consultados {num_docs} documentos jur√≠dicos (RAG/FAISS)\n\n"

            # ETAPA 6: Montar Prompt Markdown e Chamar LLM
            resposta_markdown, prompt_enviado = self._gerar_resposta_markdown_llm(
                request.promptUsuario,
                query_normalizada,
                chunks_agrupados,
                history=history_dicts,
                return_prompt=True
            )

            # Log do prompt e resposta do LLM
            req_logger.log_llm_prompt(prompt_enviado)
            req_logger.log_llm_response(resposta_markdown)

            final_response = cabecalho + resposta_markdown
            req_logger.log_final_response(final_response)

            req_logger.add_metadata("num_chunks", len(chunks_recuperados))
            req_logger.add_metadata("num_docs", num_docs)
            req_logger.add_metadata("llm_provider", self.provider)
            req_logger.add_metadata("llm_model", self.model)

            log.info("Consulta RAG (Markdown) processada com sucesso")
            req_logger.save()

            return final_response

        except Exception as e:
            req_logger.log_error(str(e), "query_markdown")
            req_logger.save()
            raise
    
    def _gerar_resposta_markdown_llm(
        self,
        query_original: str,
        query_normalizada: QueryNormalizadaOutput,
        chunks_agrupados: Dict[str, List[ChunkWithScore]],
        history: List[Dict[str, str]] = None,
        return_prompt: bool = False
    ):
        """
        Gera resposta em Markdown usando LLM com template UX jur√≠dica.

        Args:
            return_prompt: Se True, retorna tupla (resposta, prompt) para logging
        """

        # Formata dados de execu√ß√£o penal
        dados_exec = query_normalizada.dadosExecucaoPenal
        dados_exec_str = json.dumps(dados_exec.dict(), ensure_ascii=False, indent=2)

        # Formata temas e palavras-chave
        temas_str = ", ".join(query_normalizada.temaExecucao) if query_normalizada.temaExecucao else "Nenhum tema espec√≠fico identificado"
        palavras_str = ", ".join(query_normalizada.palavrasChaveJuridicas) if query_normalizada.palavrasChaveJuridicas else "Nenhuma palavra-chave espec√≠fica"

        # Monta contexto dos documentos
        contexto_docs = montar_contexto_documentos(chunks_agrupados)

        # Formata hist√≥rico de conversa
        historico_str = ""
        if history and len(history) > 0:
            historico_str = "**HIST√ìRICO DA CONVERSA:**\n"
            for msg in history:
                role_label = "Usu√°rio" if msg.get("role") == "user" else "Assistente"
                content = msg.get("content", "")
                # Limita tamanho do hist√≥rico para n√£o estourar contexto
                if len(content) > 500:
                    content = content[:500] + "..."
                historico_str += f"- **{role_label}:** {content}\n"
            historico_str += "\n"

        # Monta prompt final com template Markdown
        prompt = TEMPLATE_RAG_SEEU_MARKDOWN.format(
            query_original=query_original,
            query_normalizada=query_normalizada.queryRAG,
            historico_conversa=historico_str,
            dados_execucao=dados_exec_str,
            temas_execucao=temas_str,
            palavras_chave=palavras_str,
            documentos_contexto=contexto_docs
        )

        log.debug(f"Prompt Markdown montado ({len(prompt)} chars)")

        # Chama LLM e retorna Markdown direto
        resposta_markdown = self._chamar_llm(prompt)

        # Remove poss√≠veis markdown fences se o LLM insistir em adicionar
        resposta_limpa = resposta_markdown.strip()
        if resposta_limpa.startswith("```markdown"):
            resposta_limpa = resposta_limpa[11:]
        if resposta_limpa.startswith("```"):
            resposta_limpa = resposta_limpa[3:]
        if resposta_limpa.endswith("```"):
            resposta_limpa = resposta_limpa[:-3]

        resposta_final = resposta_limpa.strip()

        if return_prompt:
            return resposta_final, prompt
        return resposta_final
    
    def _resposta_markdown_sem_rag(
        self,
        request: RagQueryRequest,
        query_normalizada: QueryNormalizadaOutput
    ) -> str:
        """Resposta em Markdown quando useRag=False."""
        return """## Modo Chat Simples Ativado

Esta consulta foi realizada **sem utilizar a base de conhecimento jur√≠dica** (RAG desativado).

Para respostas fundamentadas em jurisprud√™ncia, ative o modo **Base de Conhecimento** na interface.

## Avisos e limita√ß√µes

- Resposta gerada diretamente pelo modelo de linguagem sem consulta √† base jur√≠dica.
- N√£o utiliza documentos indexados do STJ, STF ou outros tribunais.
- Para an√°lises fundamentadas, recomenda-se ativar o modo RAG.
"""
    
    def _resposta_markdown_vazia(
        self,
        request: RagQueryRequest,
        query_normalizada: QueryNormalizadaOutput
    ) -> str:
        """Resposta em Markdown quando nenhum chunk √© recuperado."""
        return f"""## Resumo objetivo

- Nenhum documento relevante foi encontrado na base de conhecimento para a consulta: "{request.promptUsuario[:100]}..."
- A base de dados pode n√£o conter jurisprud√™ncia espec√≠fica sobre este tema.
- Considere reformular a pergunta com termos jur√≠dicos mais espec√≠ficos.

## O que os documentos analisados tratam

Nenhum documento foi recuperado da base de dados.

## Conclus√£o

- N√£o foi poss√≠vel localizar jurisprud√™ncia relevante na base indexada.
- Isso pode indicar:
  - Tema muito espec√≠fico ou recente sem precedentes indexados.
  - Necessidade de reformula√ß√£o da consulta com termos mais t√©cnicos.
  - Limita√ß√£o do corpus de documentos dispon√≠vel.

## Jurisprud√™ncias utilizadas

Nenhuma jurisprud√™ncia foi utilizada (nenhum documento encontrado).

## Pr√≥ximos passos sugeridos

- Reformular a consulta utilizando terminologia jur√≠dica mais espec√≠fica
- Consultar diretamente os sites dos tribunais (STJ, STF, TJs)
- Verificar a LEP (Lei de Execu√ß√£o Penal) para embasamento legal
- Considerar busca manual por precedentes similares
- Entrar em contato com o suporte t√©cnico se acreditar que o documento deveria estar dispon√≠vel

## Avisos e limita√ß√µes

- Esta resposta indica aus√™ncia de documentos na base de dados para os termos consultados.
- N√£o substitui an√°lise t√©cnico-jur√≠dica completa do processo.
- A base de conhecimento √© limitada aos documentos indexados at√© o momento.
"""

