"""
Servi√ßo RAG Orquestrador para consultas jur√≠dicas SEEU.
Coordena: normaliza√ß√£o ‚Üí busca vetorial ‚Üí LLM ‚Üí resposta estruturada.
"""
import os
import json
import logging
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

from openai import OpenAI
from anthropic import Anthropic

from src.rag_schemas import (
    RagQueryRequest,
    RagQueryResponse,
    QueryNormalizadaOutput,
    ChunkWithScore,
    ChunkMetadata,
    TeseJuridica,
    JurisprudenciaReferencia
)
from src.rag_normalizer import get_normalizer
from src.storage.base import VectorStore
from src.schema import SearchResult
from src import embeddings

log = logging.getLogger(__name__)


# ========================================
# TEMPLATE DO PROMPT FINAL SEEU
# ========================================

TEMPLATE_RAG_SEEU = """Voc√™ √© um assistente jur√≠dico especializado em **execu√ß√£o penal** e no sistema **SEEU** (Sistema Eletr√¥nico de Execu√ß√£o Unificado).

**CONTEXTO DA CONSULTA:**
Query original do usu√°rio: "{query_original}"
Query normalizada (t√©cnica): "{query_normalizada}"

**DADOS DE EXECU√á√ÉO PENAL IDENTIFICADOS:**
{dados_execucao}

**TEMAS RELACIONADOS:**
{temas_execucao}

**PALAVRAS-CHAVE JUR√çDICAS:**
{palavras_chave}

**DOCUMENTOS JURISPRUDENCIAIS RECUPERADOS:**

{documentos_contexto}

---

**SUA TAREFA:**
Com base EXCLUSIVAMENTE nos documentos acima e nos dados de execu√ß√£o penal fornecidos, elabore uma an√°lise jur√≠dica estruturada.

**ESTRUTURA DA RESPOSTA (JSON):**

{{
  "contexto_seeu": "<Explique brevemente o contexto da execu√ß√£o penal e como o SEEU se relaciona com o caso>",
  
  "teses": [
    {{
      "titulo": "<T√≠tulo da tese jur√≠dica>",
      "descricao": "<Explica√ß√£o detalhada da tese com base na jurisprud√™ncia>",
      "documentosSuporte": [<lista de IDs dos documentos que sustentam esta tese>]
    }}
  ],
  
  "aplicacao_caso": "<Aplica√ß√£o pr√°tica ao caso concreto, considerando os dados de execu√ß√£o penal fornecidos>",
  
  "jurisprudencias": [
    {{
      "docId": <ID do documento>,
      "tribunal": "<Tribunal>",
      "processo": "<N√∫mero do processo>",
      "ano": <Ano da decis√£o>,
      "tema": "<Tema principal>",
      "relevanciaRelativa": <Relev√¢ncia em %>,
      "trechoUtilizado": "<Trecho espec√≠fico que fundamenta a an√°lise>"
    }}
  ],
  
  "avisos_limitacoes": "<Avisos sobre limita√ß√µes da an√°lise e car√°ter meramente informativo>"
}}

**REGRAS CR√çTICAS:**
1. Use APENAS informa√ß√µes presentes nos documentos fornecidos
2. N√ÉO invente n√∫meros de processo, datas ou fatos
3. Para cada tese, cite os documentos que a sustentam (use os IDs: Documento 1, 2, 3...)
4. Na se√ß√£o "jurisprudencias", inclua TODOS os documentos relevantes com seus trechos
5. A relev√¢ncia relativa j√° est√° calculada - use o valor fornecido
6. Se faltarem informa√ß√µes de execu√ß√£o penal, mencione isso em "avisos_limitacoes"
7. Mantenha linguagem t√©cnico-jur√≠dica mas compreens√≠vel

Retorne apenas o JSON (sem markdown):"""


# ========================================
# TEMPLATE MARKDOWN PARA UX JUR√çDICA SEEU
# ========================================

TEMPLATE_RAG_SEEU_MARKDOWN = """Voc√™ √© um assistente jur√≠dico especializado em **execu√ß√£o penal** e no sistema **SEEU**.

**CONTEXTO DA CONSULTA:**
- Query original: "{query_original}"
- Query normalizada: "{query_normalizada}"

**DADOS DE EXECU√á√ÉO PENAL IDENTIFICADOS:**
{dados_execucao}

**TEMAS RELACIONADOS:** {temas_execucao}

**PALAVRAS-CHAVE JUR√çDICAS:** {palavras_chave}

**DOCUMENTOS JURISPRUDENCIAIS RECUPERADOS:**

{documentos_contexto}

---

**SUA TAREFA:**
Gerar uma resposta em Markdown LIMPO e BEM FORMATADO, seguindo EXATAMENTE a estrutura abaixo.

**REGRAS DE FORMATA√á√ÉO CR√çTICAS:**
1. Use quebras de linha duplas entre se√ß√µes (\\n\\n)
2. Use bullets (-) para listas
3. Use **negrito** para destacar informa√ß√µes importantes
4. Mantenha par√°grafos curtos e diretos
5. Separe visualmente cada jurisprud√™ncia

---

## üìã Resumo Objetivo

- [Bullet 1: Resposta direta √† pergunta do usu√°rio]
- [Bullet 2: Principais conclus√µes baseadas nos documentos]
- [Bullet 3: Limita√ß√µes ou observa√ß√µes importantes]

---

## üìö Documentos Analisados

**Documento 1 ‚Äì [TRIBUNAL] ‚Äì [PROCESSO] ‚Äì [ANO]**
- **Relev√¢ncia:** XX.X%
- **Tema:** [Resumo em 1 linha do tema central]

**Documento 2 ‚Äì [TRIBUNAL] ‚Äì [PROCESSO] ‚Äì [ANO]**
- **Relev√¢ncia:** XX.X%
- **Tema:** [Resumo em 1 linha do tema central]

[Repita para cada documento]

---

## ‚öñÔ∏è Jurisprud√™ncias Relevantes

### üìå [TRIBUNAL] ‚Äì Processo n¬∫ [N√öMERO]

**üìä Relev√¢ncia:** XX.X%  
**üìÖ Ano:** AAAA  
**üèõÔ∏è Relator(a):** [Nome do relator se dispon√≠vel]  
**üìë Tema:** [Tema principal da decis√£o]

**üí° Trecho Relevante:**
> "[Trecho espec√≠fico mais importante que fundamenta a an√°lise, entre 2-4 linhas]"

---

### üìå [TRIBUNAL] ‚Äì Processo n¬∫ [N√öMERO]

[Repita a mesma estrutura para cada jurisprud√™ncia]

---

## ‚úÖ Conclus√£o

- [Bullet 1: √â poss√≠vel ou n√£o responder √† pergunta com base nos documentos?]
- [Bullet 2: O que FALTA de informa√ß√£o, se for o caso]
- [Bullet 3: Leitura mais prudente diante da jurisprud√™ncia encontrada]
- [Bullet 4: Recomenda√ß√µes pr√°ticas para o caso]

---

## üéØ Pr√≥ximos Passos Sugeridos

1. **Legisla√ß√£o:**
   - Consultar LEP, artigos [X, Y, Z] que tratam de [tema]
   
2. **Pesquisa Complementar:**
   - Buscar jurisprud√™ncia espec√≠fica no [TRIBUNAL] sobre "[palavras-chave]"
   
3. **Dados do Caso:**
   - Obter informa√ß√£o sobre [dado espec√≠fico necess√°rio]
   
4. **Documenta√ß√£o:**
   - Reunir documentos comprobat√≥rios de [requisito espec√≠fico]

---

## ‚ö†Ô∏è Avisos e Limita√ß√µes

- ‚úì Esta resposta tem car√°ter **informativo e consultivo**
- ‚úì **N√ÉO substitui** an√°lise t√©cnico-jur√≠dica completa do processo
- ‚úì Baseada **exclusivamente** nos documentos retornados pelo sistema
- ‚úì Recomenda-se consulta aos autos originais e verifica√ß√£o de jurisprud√™ncia mais recente

---

**REGRAS CR√çTICAS:**
1. Baseie-se APENAS nos documentos fornecidos
2. N√ÉO invente n√∫meros de processos, tribunais, anos ou URLs
3. Se faltar informa√ß√£o, diga explicitamente o que falta
4. Mantenha SEMPRE a estrutura das se√ß√µes conforme descrito acima
5. Use negrito para destacar pontos cr√≠ticos e palavras-chave
6. Seja direto, mas respeitoso e t√©cnico
7. Quando os documentos N√ÉO trazem a resposta, seja transparente
8. A relev√¢ncia relativa j√° est√° calculada - use o valor fornecido

Retorne APENAS o texto em Markdown (sem c√≥digo markdown com ```):"""


# ========================================
# FUN√á√ïES AUXILIARES
# ========================================

def calcular_relevancia_relativa(scores: List[float]) -> List[float]:
    """
    Calcula relev√¢ncia em porcentagem diretamente do score.
    
    Args:
        scores: Lista de scores brutos de similaridade (0 a 1)
        
    Returns:
        Lista de relev√¢ncias em porcentagem (score * 100)
    """
    if not scores:
        return []
    
    # Converte score diretamente para porcentagem
    relevancia_relativa = [score * 100 for score in scores]
    
    return relevancia_relativa


def agrupar_chunks_por_documento(
    chunks: List[ChunkWithScore]
) -> Dict[str, List[ChunkWithScore]]:
    """
    Agrupa chunks pelo ID do documento global.
    
    Returns:
        Dict com chave = idDocumentoGlobal, valor = lista de chunks
    """
    docs: Dict[str, List[ChunkWithScore]] = {}
    
    for chunk in chunks:
        doc_id = chunk.metadata.idDocumentoGlobal
        if doc_id not in docs:
            docs[doc_id] = []
        docs[doc_id].append(chunk)
    
    return docs


def montar_contexto_documentos(
    chunks_agrupados: Dict[str, List[ChunkWithScore]]
) -> str:
    """
    Monta string de contexto formatado para o LLM.
    
    Cada documento √© numerado sequencialmente com metadados completos.
    """
    contexto_partes = []
    doc_numero = 1
    
    for doc_id, chunks in chunks_agrupados.items():
        # Ordena chunks por posi√ß√£o se dispon√≠vel
        chunks_ordenados = sorted(
            chunks,
            key=lambda c: c.metadata.posicaoChunk or 0
        )
        
        # Pega metadados do primeiro chunk
        meta = chunks_ordenados[0].metadata
        
        # Cabe√ßalho do documento com formata√ß√£o melhorada
        contexto_partes.append(f"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        contexto_partes.append(f"üìÑ DOCUMENTO {doc_numero}")
        contexto_partes.append(f"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        
        if meta.tribunal:
            contexto_partes.append(f"üèõÔ∏è  Tribunal: {meta.tribunal}")
        if meta.numeroProcesso:
            contexto_partes.append(f"üìã Processo: {meta.numeroProcesso}")
        if meta.relator:
            contexto_partes.append(f"üë§ Relator(a): {meta.relator}")
        if meta.dataJulgamento:
            contexto_partes.append(f"üìÖ Data Julgamento: {meta.dataJulgamento}")
        if meta.orgaoJulgador:
            contexto_partes.append(f"‚öñÔ∏è  √ìrg√£o Julgador: {meta.orgaoJulgador}")
        if meta.tema:
            contexto_partes.append(f"üîñ Tema: {meta.tema}")
        
        contexto_partes.append("")
        contexto_partes.append("üìù TRECHOS RELEVANTES:")
        contexto_partes.append("")
        
        # Chunks do documento
        for i, chunk in enumerate(chunks_ordenados, 1):
            relevancia = chunk.relevanciaRelativa or (chunk.score * 100)
            contexto_partes.append(f"‚ñ∏ Trecho {i} (Relev√¢ncia: {relevancia:.1f}%):")
            contexto_partes.append(f'  "{chunk.texto}"')
            contexto_partes.append("")
        
        doc_numero += 1
    
    return "\n".join(contexto_partes)


# ========================================
# SERVI√áO RAG
# ========================================

class RagService:
    """Servi√ßo orquestrador de RAG jur√≠dico."""
    
    def __init__(
        self,
        store: VectorStore,
        provider: str = "openai",
        model: Optional[str] = None,
        api_key: Optional[str] = None
    ):
        """
        Inicializa o servi√ßo RAG.
        
        Args:
            store: Store vetorial (FAISS, OpenSearch, etc.)
            provider: Provedor do LLM ("openai" ou "anthropic")
            model: Nome do modelo
            api_key: Chave da API
        """
        self.store = store
        self.provider = provider.lower()
        
        # Configura√ß√£o do LLM
        if self.provider == "openai":
            self.model = model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            api_key = api_key or os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY n√£o configurada")
            self.client = OpenAI(api_key=api_key)
            
        elif self.provider == "anthropic":
            self.model = model or os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet-20241022")
            api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEY n√£o configurada")
            self.client = Anthropic(api_key=api_key)
        else:
            raise ValueError(f"Provider n√£o suportado: {provider}")
        
        log.info(f"RagService inicializado: {self.provider} / {self.model}")
    
    def processar_consulta(
        self,
        request: RagQueryRequest
    ) -> RagQueryResponse:
        """
        Processa consulta RAG completa.
        
        Fluxo:
        1. Normaliza√ß√£o jur√≠dica
        2. Busca vetorial
        3. C√°lculo de relev√¢ncia relativa
        4. Constru√ß√£o do prompt
        5. Chamada ao LLM
        6. Parse e estrutura√ß√£o da resposta
        
        Args:
            request: Request com prompt do usu√°rio e metadados
            
        Returns:
            RagQueryResponse estruturada
        """
        log.info(f"Processando consulta RAG: {request.promptUsuario[:100]}...")
        
        # ETAPA 1: Normaliza√ß√£o Jur√≠dica
        normalizer = get_normalizer()
        contexto_meta = self._formatar_contexto_metadados(request.metadados)
        query_normalizada = normalizer.normalizar(
            prompt_usuario=request.promptUsuario,
            contexto_adicional=contexto_meta
        )
        
        log.info(f"Query normalizada: {query_normalizada.queryRAG}")
        
        # ETAPA 2: Busca Vetorial
        if not request.useRag:
            # Modo sem RAG - retorna resposta direta (TODO: implementar)
            return self._resposta_sem_rag(request, query_normalizada)
        
        chunks_recuperados = self._buscar_chunks(
            query_normalizada.queryRAG,
            k=request.k,
            metadados=request.metadados
        )
        
        if not chunks_recuperados:
            log.warning("Nenhum chunk recuperado")
            return self._resposta_vazia(request, query_normalizada)
        
        # ETAPA 3: Calcular Relev√¢ncia Relativa
        scores = [c.score for c in chunks_recuperados]
        relevancia_relativa = calcular_relevancia_relativa(scores)
        
        for i, chunk in enumerate(chunks_recuperados):
            chunk.relevanciaRelativa = round(relevancia_relativa[i], 1)
        
        # ETAPA 4: Agrupar por Documento
        chunks_agrupados = agrupar_chunks_por_documento(chunks_recuperados)
        
        log.info(
            f"Recuperados {len(chunks_recuperados)} chunks de "
            f"{len(chunks_agrupados)} documentos √∫nicos"
        )
        
        # ETAPA 5: Montar Prompt e Chamar LLM
        resposta_llm = self._gerar_resposta_llm(
            request.promptUsuario,
            query_normalizada,
            chunks_agrupados
        )
        
        # ETAPA 6: Estruturar Resposta Final
        resposta_final = self._estruturar_resposta(
            request,
            query_normalizada,
            chunks_recuperados,
            chunks_agrupados,
            resposta_llm
        )
        
        log.info("Consulta RAG processada com sucesso")
        return resposta_final
    
    def _formatar_contexto_metadados(self, metadados) -> str:
        """Formata metadados para contexto do normalizador."""
        if not metadados:
            return "Nenhum metadado adicional."
        
        partes = []
        if metadados.tribunal:
            partes.append(f"Tribunal: {metadados.tribunal}")
        if metadados.anoMin or metadados.anoMax:
            partes.append(f"Per√≠odo: {metadados.anoMin or '?'} - {metadados.anoMax or '?'}")
        if metadados.tipoConsulta:
            partes.append(f"Tipo: {metadados.tipoConsulta}")
        
        return " | ".join(partes) if partes else "Nenhum metadado adicional."
    
    def _buscar_chunks(
        self,
        query: str,
        k: int,
        metadados
    ) -> List[ChunkWithScore]:
        """
        Executa busca vetorial e converte para ChunkWithScore.
        
        TODO: Implementar filtros de metadados quando store suportar.
        """
        # Gera embedding
        query_vector = embeddings.encode_single_text(query)
        
        # Busca no store
        resultados: List[SearchResult] = self.store.search(query_vector, k=k)
        
        # Converte para ChunkWithScore
        chunks = []
        for resultado in resultados:
            doc = resultado.doc
            
            # Extrai metadados do chunk
            meta_dict = doc.meta or {}
            metadata = ChunkMetadata(
                idDocumentoGlobal=meta_dict.get("idDocumentoGlobal", doc.id),
                idChunk=doc.id,
                tribunal=doc.court or meta_dict.get("tribunal"),
                numeroProcesso=meta_dict.get("numeroProcesso"),
                orgaoJulgador=meta_dict.get("orgaoJulgador"),
                relator=meta_dict.get("relator"),
                dataJulgamento=doc.date or meta_dict.get("dataJulgamento"),
                tema=meta_dict.get("tema"),
                fonte=meta_dict.get("fonte"),
                posicaoChunk=meta_dict.get("posicaoChunk"),
                totalChunks=meta_dict.get("totalChunks")
            )
            
            chunk = ChunkWithScore(
                texto=doc.text,
                metadata=metadata,
                score=resultado.score
            )
            chunks.append(chunk)
        
        return chunks
    
    def _gerar_resposta_llm(
        self,
        query_original: str,
        query_normalizada: QueryNormalizadaOutput,
        chunks_agrupados: Dict[str, List[ChunkWithScore]]
    ) -> Dict[str, Any]:
        """Gera resposta estruturada usando LLM."""
        
        # Formata dados de execu√ß√£o penal
        dados_exec = query_normalizada.dadosExecucaoPenal
        dados_exec_str = json.dumps(dados_exec.dict(), ensure_ascii=False, indent=2)
        
        # Formata temas e palavras-chave
        temas_str = ", ".join(query_normalizada.temaExecucao) if query_normalizada.temaExecucao else "Nenhum tema espec√≠fico identificado"
        palavras_str = ", ".join(query_normalizada.palavrasChaveJuridicas) if query_normalizada.palavrasChaveJuridicas else "Nenhuma palavra-chave espec√≠fica"
        
        # Monta contexto dos documentos
        contexto_docs = montar_contexto_documentos(chunks_agrupados)
        
        # Monta prompt final
        prompt = TEMPLATE_RAG_SEEU.format(
            query_original=query_original,
            query_normalizada=query_normalizada.queryRAG,
            dados_execucao=dados_exec_str,
            temas_execucao=temas_str,
            palavras_chave=palavras_str,
            documentos_contexto=contexto_docs
        )
        
        log.debug(f"Prompt final montado ({len(prompt)} chars)")
        
        # Chama LLM
        resposta_raw = self._chamar_llm(prompt)
        
        # Parse JSON
        return self._parse_resposta_llm(resposta_raw)
    
    def _chamar_llm(self, prompt: str) -> str:
        """Chama o LLM apropriado."""
        if self.provider == "openai":
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Voc√™ √© um assistente jur√≠dico especializado em execu√ß√£o penal. Retorne apenas JSON v√°lido."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.4,
                max_tokens=4000
            )
            return response.choices[0].message.content.strip()
        
        elif self.provider == "anthropic":
            response = self.client.messages.create(
                model=self.model,
                max_tokens=4000,
                temperature=0.4,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return response.content[0].text.strip()
        
        raise ValueError(f"Provider n√£o suportado: {self.provider}")
    
    def _parse_resposta_llm(self, resposta_raw: str) -> Dict[str, Any]:
        """Parse da resposta JSON do LLM."""
        # Remove markdown fences
        resposta_limpa = resposta_raw.strip()
        if resposta_limpa.startswith("```json"):
            resposta_limpa = resposta_limpa[7:]
        if resposta_limpa.startswith("```"):
            resposta_limpa = resposta_limpa[3:]
        if resposta_limpa.endswith("```"):
            resposta_limpa = resposta_limpa[:-3]
        resposta_limpa = resposta_limpa.strip()
        
        try:
            return json.loads(resposta_limpa)
        except json.JSONDecodeError as e:
            log.error(f"Erro ao fazer parse do JSON do LLM: {e}")
            log.error(f"Resposta recebida: {resposta_raw[:500]}")
            raise ValueError(f"LLM retornou JSON inv√°lido: {e}")
    
    def _estruturar_resposta(
        self,
        request: RagQueryRequest,
        query_normalizada: QueryNormalizadaOutput,
        chunks_todos: List[ChunkWithScore],
        chunks_agrupados: Dict[str, List[ChunkWithScore]],
        resposta_llm: Dict[str, Any]
    ) -> RagQueryResponse:
        """Estrutura resposta final no formato RagQueryResponse."""
        
        # Extrai campos do LLM
        teses_llm = resposta_llm.get("teses", [])
        jurisp_llm = resposta_llm.get("jurisprudencias", [])
        
        # Converte para Pydantic
        teses = [TeseJuridica(**t) for t in teses_llm]
        jurisprudencias = [JurisprudenciaReferencia(**j) for j in jurisp_llm]
        
        return RagQueryResponse(
            queryOriginal=request.promptUsuario,
            queryNormalizada=query_normalizada,
            timestampConsulta=datetime.utcnow().isoformat(),
            contexto_seeu=resposta_llm.get("contexto_seeu", ""),
            teses=teses,
            aplicacao_caso=resposta_llm.get("aplicacao_caso", ""),
            jurisprudencias=jurisprudencias,
            avisos_limitacoes=resposta_llm.get("avisos_limitacoes", ""),
            backend=self.store.__class__.__name__,
            totalChunksRecuperados=len(chunks_todos),
            totalDocumentosUnicos=len(chunks_agrupados)
        )
    
    def _resposta_sem_rag(
        self,
        request: RagQueryRequest,
        query_normalizada: QueryNormalizadaOutput
    ) -> RagQueryResponse:
        """Resposta quando useRag=False (TODO: implementar)."""
        return RagQueryResponse(
            queryOriginal=request.promptUsuario,
            queryNormalizada=query_normalizada,
            contexto_seeu="Modo sem RAG - resposta direta (n√£o implementado)",
            teses=[],
            aplicacao_caso="",
            jurisprudencias=[],
            avisos_limitacoes="Funcionalidade em desenvolvimento",
            backend="none",
            totalChunksRecuperados=0,
            totalDocumentosUnicos=0
        )
    
    def _resposta_vazia(
        self,
        request: RagQueryRequest,
        query_normalizada: QueryNormalizadaOutput
    ) -> RagQueryResponse:
        """Resposta quando nenhum chunk √© recuperado."""
        return RagQueryResponse(
            queryOriginal=request.promptUsuario,
            queryNormalizada=query_normalizada,
            contexto_seeu="Nenhum documento relevante encontrado na base de dados.",
            teses=[],
            aplicacao_caso="",
            jurisprudencias=[],
            avisos_limitacoes="N√£o foram encontrados documentos jurisprudenciais relevantes para esta consulta. Considere reformular a pergunta ou verificar se a base de dados cont√©m informa√ß√µes sobre o tema.",
            backend=self.store.__class__.__name__,
            totalChunksRecuperados=0,
            totalDocumentosUnicos=0
        )

    def query_markdown(
        self,
        request: RagQueryRequest
    ) -> str:
        """
        Processa consulta RAG e retorna resposta em Markdown puro (formato UX jur√≠dica).
        
        Este m√©todo √© otimizado para exibi√ß√£o direta na interface do usu√°rio,
        seguindo as diretrizes de UX jur√≠dica do SEEU.
        
        Fluxo:
        1. Normaliza√ß√£o jur√≠dica
        2. Busca vetorial
        3. C√°lculo de relev√¢ncia relativa
        4. Constru√ß√£o do prompt Markdown
        5. Chamada ao LLM
        6. Retorno direto do Markdown gerado
        
        Args:
            request: Request com prompt do usu√°rio e metadados
            
        Returns:
            String em Markdown formatado para operadores do direito
        """
        log.info(f"Processando consulta RAG (Markdown): {request.promptUsuario[:100]}...")
        
        # ETAPA 1: Normaliza√ß√£o Jur√≠dica
        normalizer = get_normalizer()
        contexto_meta = self._formatar_contexto_metadados(request.metadados)
        query_normalizada = normalizer.normalizar(
            prompt_usuario=request.promptUsuario,
            contexto_adicional=contexto_meta
        )
        
        log.info(f"Query normalizada: {query_normalizada.queryRAG}")
        
        # ETAPA 2: Busca Vetorial
        if not request.useRag:
            # Modo sem RAG - retorna resposta direta
            return self._resposta_markdown_sem_rag(request, query_normalizada)
        
        chunks_recuperados = self._buscar_chunks(
            query_normalizada.queryRAG,
            k=request.k,
            metadados=request.metadados
        )
        
        if not chunks_recuperados:
            log.warning("Nenhum chunk recuperado")
            return self._resposta_markdown_vazia(request, query_normalizada)
        
        # ETAPA 3: Calcular Relev√¢ncia Relativa
        scores = [c.score for c in chunks_recuperados]
        relevancia_relativa = calcular_relevancia_relativa(scores)
        
        for i, chunk in enumerate(chunks_recuperados):
            chunk.relevanciaRelativa = round(relevancia_relativa[i], 1)
        
        # ETAPA 4: Agrupar por Documento
        chunks_agrupados = agrupar_chunks_por_documento(chunks_recuperados)
        
        log.info(
            f"Recuperados {len(chunks_recuperados)} chunks de "
            f"{len(chunks_agrupados)} documentos √∫nicos"
        )
        
        # ETAPA 5: Gerar cabe√ßalho informativo
        num_docs = len(chunks_agrupados)
        cabecalho = f"üìö Consultados {num_docs} documentos jur√≠dicos (RAG/FAISS)\n\n"
        
        # ETAPA 6: Montar Prompt Markdown e Chamar LLM
        resposta_markdown = self._gerar_resposta_markdown_llm(
            request.promptUsuario,
            query_normalizada,
            chunks_agrupados
        )
        
        log.info("Consulta RAG (Markdown) processada com sucesso")
        return cabecalho + resposta_markdown
    
    def _gerar_resposta_markdown_llm(
        self,
        query_original: str,
        query_normalizada: QueryNormalizadaOutput,
        chunks_agrupados: Dict[str, List[ChunkWithScore]]
    ) -> str:
        """Gera resposta em Markdown usando LLM com template UX jur√≠dica."""
        
        # Formata dados de execu√ß√£o penal
        dados_exec = query_normalizada.dadosExecucaoPenal
        dados_exec_str = json.dumps(dados_exec.dict(), ensure_ascii=False, indent=2)
        
        # Formata temas e palavras-chave
        temas_str = ", ".join(query_normalizada.temaExecucao) if query_normalizada.temaExecucao else "Nenhum tema espec√≠fico identificado"
        palavras_str = ", ".join(query_normalizada.palavrasChaveJuridicas) if query_normalizada.palavrasChaveJuridicas else "Nenhuma palavra-chave espec√≠fica"
        
        # Monta contexto dos documentos
        contexto_docs = montar_contexto_documentos(chunks_agrupados)
        
        # Monta prompt final com template Markdown
        prompt = TEMPLATE_RAG_SEEU_MARKDOWN.format(
            query_original=query_original,
            query_normalizada=query_normalizada.queryRAG,
            dados_execucao=dados_exec_str,
            temas_execucao=temas_str,
            palavras_chave=palavras_str,
            documentos_contexto=contexto_docs
        )
        
        log.debug(f"Prompt Markdown montado ({len(prompt)} chars)")
        
        # Chama LLM e retorna Markdown direto
        resposta_markdown = self._chamar_llm(prompt)
        
        # Remove poss√≠veis markdown fences se o LLM insistir em adicionar
        resposta_limpa = resposta_markdown.strip()
        if resposta_limpa.startswith("```markdown"):
            resposta_limpa = resposta_limpa[11:]
        if resposta_limpa.startswith("```"):
            resposta_limpa = resposta_limpa[3:]
        if resposta_limpa.endswith("```"):
            resposta_limpa = resposta_limpa[:-3]
        
        return resposta_limpa.strip()
    
    def _resposta_markdown_sem_rag(
        self,
        request: RagQueryRequest,
        query_normalizada: QueryNormalizadaOutput
    ) -> str:
        """Resposta em Markdown quando useRag=False."""
        return """## Modo Chat Simples Ativado

Esta consulta foi realizada **sem utilizar a base de conhecimento jur√≠dica** (RAG desativado).

Para respostas fundamentadas em jurisprud√™ncia, ative o modo **Base de Conhecimento** na interface.

## Avisos e limita√ß√µes

- Resposta gerada diretamente pelo modelo de linguagem sem consulta √† base jur√≠dica.
- N√£o utiliza documentos indexados do STJ, STF ou outros tribunais.
- Para an√°lises fundamentadas, recomenda-se ativar o modo RAG.
"""
    
    def _resposta_markdown_vazia(
        self,
        request: RagQueryRequest,
        query_normalizada: QueryNormalizadaOutput
    ) -> str:
        """Resposta em Markdown quando nenhum chunk √© recuperado."""
        return f"""## Resumo objetivo

- Nenhum documento relevante foi encontrado na base de conhecimento para a consulta: "{request.promptUsuario[:100]}..."
- A base de dados pode n√£o conter jurisprud√™ncia espec√≠fica sobre este tema.
- Considere reformular a pergunta com termos jur√≠dicos mais espec√≠ficos.

## O que os documentos analisados tratam

Nenhum documento foi recuperado da base de dados.

## Conclus√£o

- N√£o foi poss√≠vel localizar jurisprud√™ncia relevante na base indexada.
- Isso pode indicar:
  - Tema muito espec√≠fico ou recente sem precedentes indexados.
  - Necessidade de reformula√ß√£o da consulta com termos mais t√©cnicos.
  - Limita√ß√£o do corpus de documentos dispon√≠vel.

## Jurisprud√™ncias utilizadas

Nenhuma jurisprud√™ncia foi utilizada (nenhum documento encontrado).

## Pr√≥ximos passos sugeridos

- Reformular a consulta utilizando terminologia jur√≠dica mais espec√≠fica
- Consultar diretamente os sites dos tribunais (STJ, STF, TJs)
- Verificar a LEP (Lei de Execu√ß√£o Penal) para embasamento legal
- Considerar busca manual por precedentes similares
- Entrar em contato com o suporte t√©cnico se acreditar que o documento deveria estar dispon√≠vel

## Avisos e limita√ß√µes

- Esta resposta indica aus√™ncia de documentos na base de dados para os termos consultados.
- N√£o substitui an√°lise t√©cnico-jur√≠dica completa do processo.
- A base de conhecimento √© limitada aos documentos indexados at√© o momento.
"""

