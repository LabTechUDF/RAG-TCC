# Fluxo TÃ©cnico RAG - Retrieval-Augmented Generation

## ğŸ“‹ VisÃ£o Geral TÃ©cnica

Este documento descreve em detalhes tÃ©cnicos e lÃ³gicos o fluxo completo de funcionamento do sistema RAG (Retrieval-Augmented Generation) quando o usuÃ¡rio escolhe o modo de operaÃ§Ã£o RAG na interface.

## ğŸ—ï¸ Arquitetura do Sistema

O sistema RAG Ã© composto por trÃªs componentes principais:

1. **Interface (Nuxt 3 + TypeScript)** - Frontend e API intermediÃ¡ria
2. **DBVECTOR (FastAPI + Python)** - ServiÃ§o de busca vetorial
3. **OpenAI API** - LLM para geraÃ§Ã£o de respostas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Interface  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   DBVECTOR   â”‚         â”‚   OpenAI    â”‚
â”‚  (Nuxt 3)   â”‚         â”‚   (FastAPI)  â”‚         â”‚     API     â”‚
â”‚             â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”‚              â”‚         â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                                 â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Fluxo Detalhado do RAG

### **Fase 1: InicializaÃ§Ã£o do Modo RAG**

#### 1.1 SeleÃ§Ã£o do Modo (Frontend)
**Arquivo:** `/Interface/app/pages/index.vue`

```typescript
const useRAG = ref(true) // Toggle entre RAG e chat simples
```

**LÃ³gica:**
- VariÃ¡vel reativa `useRAG` controla o modo de operaÃ§Ã£o
- `true` = Modo RAG (busca documentos + LLM)
- `false` = Chat Simples (direto para LLM)
- Interface oferece toggle visual para o usuÃ¡rio

**Estado:** O usuÃ¡rio seleciona "RAG" na interface

---

### **Fase 2: SubmissÃ£o da Query**

#### 2.1 Captura e ValidaÃ§Ã£o (Frontend)
**Arquivo:** `/Interface/app/pages/index.vue` - FunÃ§Ã£o `onSubmit()`

```typescript
function onSubmit() {
  if (input.value.trim()) {
    sendToOpenAI(input.value)
  }
}
```

**LÃ³gica:**
- Captura texto do input
- Valida que nÃ£o estÃ¡ vazio (apÃ³s trim)
- Chama funÃ§Ã£o `sendToOpenAI()` com a query

**Exemplo de Query:** 
```
"Quais sÃ£o os direitos fundamentais previstos na ConstituiÃ§Ã£o?"
```

---

### **Fase 3: Busca Vetorial (Retrieval)**

#### 3.1 VerificaÃ§Ã£o do Modo RAG (Frontend)
**Arquivo:** `/Interface/app/pages/index.vue` - FunÃ§Ã£o `sendToOpenAI()`

```typescript
if (useRAG.value) {
  // Busca documentos no DBVECTOR
  const dbvectorResponse = await $fetch<DBVectorSearchResponse>(
    '/api/dbvector/search',
    {
      method: 'POST',
      body: { q: prompt, k: 5 }
    }
  )
}
```

**LÃ³gica:**
- Verifica se `useRAG.value === true`
- Se sim, faz requisiÃ§Ã£o HTTP POST para `/api/dbvector/search`
- ParÃ¢metros:
  - `q`: Query do usuÃ¡rio (string)
  - `k`: NÃºmero de documentos a retornar (int, default=5)

---

#### 3.2 Proxy da Interface (Backend Nuxt)
**Arquivo:** `/Interface/server/api/dbvector/search.post.ts`

```typescript
export default defineEventHandler(async (event) => {
  const body = await readBody<DBVectorSearchRequest>(event)
  
  // ValidaÃ§Ã£o
  if (!body.q || !body.q.trim()) {
    throw createError({
      statusCode: 400,
      message: 'Query (q) is required'
    })
  }

  const dbvectorUrl = config.public.dbvectorApiUrl || 'http://localhost:8000'
  const searchUrl = `${dbvectorUrl}/search`
  
  const response = await $fetch<DBVectorSearchResponse>(searchUrl, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: { q: body.q, k: body.k || 5 }
  })

  return response
})
```

**LÃ³gica:**
1. Recebe requisiÃ§Ã£o do frontend
2. Valida presenÃ§a da query
3. LÃª URL do DBVECTOR de `config.public.dbvectorApiUrl`
4. Faz proxy da requisiÃ§Ã£o para o serviÃ§o DBVECTOR
5. Retorna resposta para o frontend

**PropÃ³sito do Proxy:**
- Centraliza configuraÃ§Ãµes de URL
- Adiciona logging e tratamento de erros
- Permite futuras transformaÃ§Ãµes de dados

---

#### 3.3 API DBVECTOR - Endpoint de Busca
**Arquivo:** `/DBVECTOR/src/api/main.py` - Endpoint `/search`

```python
@app.post("/search", response_model=SearchResponseAPI)
async def search_documents(request: SearchRequest):
    # ValidaÃ§Ãµes
    if store is None:
        raise HTTPException(status_code=503, detail="Store nÃ£o inicializado")
    
    if not request.q or not request.q.strip():
        raise HTTPException(status_code=422, detail="Query nÃ£o pode ser vazia")
    
    doc_count = store.get_doc_count()
    if doc_count == 0:
        raise HTTPException(status_code=404, detail="Nenhum documento indexado")
    
    # Gera embedding da query
    query_vector = embeddings.encode_single_text(request.q)
    
    # Busca documentos
    results = store.search(query_vector, k=request.k)
    
    # Converte para modelo API
    api_results = [
        SearchResultAPI(
            id=result.doc.id,
            title=result.doc.title,
            text=result.doc.text,
            court=result.doc.court,
            code=result.doc.code,
            article=result.doc.article,
            date=result.doc.date,
            meta=result.doc.meta,
            score=result.score
        )
        for result in results
    ]
    
    return SearchResponseAPI(
        query=request.q,
        total=len(api_results),
        backend=config.SEARCH_BACKEND,
        results=api_results
    )
```

**LÃ³gica Detalhada:**

1. **ValidaÃ§Ã£o de Estado:**
   - Verifica se o `store` (FAISS ou OpenSearch) estÃ¡ inicializado
   - Verifica se hÃ¡ documentos indexados (`doc_count > 0`)

2. **GeraÃ§Ã£o de Embedding:**
   - Chama `embeddings.encode_single_text(query)`
   - Usa modelo sentence-transformers (default: `all-MiniLM-L6-v2`)
   - Retorna vetor numpy de dimensÃ£o 384 (float32)
   - Embedding Ã© normalizado se `NORMALIZE_EMBEDDINGS=true`

3. **Busca Vetorial:**
   - Chama `store.search(query_vector, k=5)`
   - Store pode ser FAISS (local) ou OpenSearch (distribuÃ­do)

---

#### 3.4 GeraÃ§Ã£o de Embedding da Query
**Arquivo:** `/DBVECTOR/src/embeddings.py`

```python
def encode_single_text(text: str) -> np.ndarray:
    """
    Gera embedding para um Ãºnico texto.
    
    Returns:
        Array numpy com shape (embedding_dim,)
    """
    embeddings = encode_texts([text])
    return embeddings[0]

def encode_texts(texts: List[str]) -> np.ndarray:
    """
    Gera embeddings para lista de textos.
    
    Returns:
        Array numpy de embeddings com shape (len(texts), embedding_dim)
        Tipo: np.float32
        Normalizado se config.NORMALIZE_EMBEDDINGS=True
    """
    model = load_model()
    
    embeddings = model.encode(
        texts,
        normalize_embeddings=config.NORMALIZE_EMBEDDINGS,
        show_progress_bar=len(texts) > 10,
        convert_to_numpy=True
    )

    arr = np.array(embeddings)
    if arr.ndim == 1:
        arr = arr.reshape(1, -1)

    return arr.astype(np.float32)
```

**LÃ³gica TÃ©cnica:**

1. **Carregamento do Modelo (Singleton):**
   - Modelo Ã© carregado uma Ãºnica vez na memÃ³ria
   - Usa `SentenceTransformer` da biblioteca sentence-transformers
   - Modelo padrÃ£o: `sentence-transformers/all-MiniLM-L6-v2`
   - DimensÃ£o: 384
   - Armazenado em cache na GPU se disponÃ­vel

2. **Encoding:**
   - TokenizaÃ§Ã£o do texto
   - Passagem pela rede neural (BERT-based)
   - Pooling (mean pooling) para gerar vetor de dimensÃ£o fixa
   - NormalizaÃ§Ã£o L2 (opcional, padrÃ£o=true)
   - ConversÃ£o para numpy float32

3. **Output:**
   - Vetor numpy de shape `(384,)`
   - Tipo: `np.float32`
   - Normalizado: `||v|| = 1.0` (se configurado)

**Exemplo:**
```python
# Input: "direitos fundamentais"
# Output: array([0.123, -0.456, 0.789, ..., 0.234], dtype=float32)
# Shape: (384,)
```

---

#### 3.5 Busca no Ãndice FAISS
**Arquivo:** `/DBVECTOR/src/storage/faiss_store.py`

```python
def search(self, query_vector: np.ndarray, k: int = 5) -> List[SearchResult]:
    """Busca documentos similares."""
    if self._index is None or self._index.ntotal == 0:
        return []
    
    # Garante que query_vector Ã© 2D
    if query_vector.ndim == 1:
        query_vector = query_vector.reshape(1, -1)
    
    # Busca no FAISS
    scores, internal_ids = self._index.search(query_vector, k)

    results = []
    for score, internal_id in zip(scores[0], internal_ids[0]):
        if internal_id == -1:  # ID invÃ¡lido
            continue
            
        if internal_id in self.metadata:
            doc_data = self.metadata[internal_id]
            doc = Doc(
                id=doc_data['id'],
                text=doc_data['text'],
                title=doc_data['title'],
                court=doc_data['court'],
                code=doc_data['code'],
                article=doc_data['article'],
                date=doc_data['date'],
                meta=doc_data['meta']
            )
            results.append(SearchResult(doc=doc, score=float(score)))
    
    return results
```

**LÃ³gica TÃ©cnica do FAISS:**

1. **PreparaÃ§Ã£o do Vetor:**
   - Converte vetor 1D para 2D: `(384,)` â†’ `(1, 384)`
   - FAISS requer arrays 2D (batch)

2. **Busca por Similaridade:**
   - Usa `IndexFlatIP` (Inner Product)
   - Com vetores normalizados, IP = cosine similarity
   - Retorna top-k documentos mais similares
   - Algoritmo: forÃ§a bruta (exato, nÃ£o aproximado)
   - Complexidade: O(n * d) onde n = docs, d = dimensÃ£o

3. **Scores:**
   - Range: [-1, 1] se normalizado (cosine)
   - Maior = mais similar
   - Threshold tÃ­pico: > 0.5 para relevÃ¢ncia

4. **RecuperaÃ§Ã£o de Metadados:**
   - FAISS armazena apenas vetores
   - Metadados (title, text, court, etc.) em Parquet separado
   - Lookup por `internal_id` (hash do doc.id)

5. **Estrutura do Ãndice:**
   ```python
   # Estrutura em memÃ³ria
   self._index = faiss.IndexIDMap2(faiss.IndexFlatIP(384))
   # Metadados
   self.metadata = {
       internal_id: {
           'id': 'doc_123',
           'text': 'conteÃºdo...',
           'title': 'tÃ­tulo',
           'court': 'STF',
           'code': 'CF',
           'article': '5Âº',
           'date': '2024-01-15',
           'meta': {}
       }
   }
   ```

6. **GPU Support (Opcional):**
   - Se `USE_FAISS_GPU=true` e GPU disponÃ­vel
   - Ãndice Ã© movido para GPU na inicializaÃ§Ã£o
   - Acelera busca em grandes volumes (>100k docs)

**Exemplo de Busca:**
```python
# Input
query_vector = [0.123, -0.456, ..., 0.234]  # shape (384,)
k = 5

# FAISS Search
scores = [0.89, 0.85, 0.82, 0.78, 0.75]  # similarity scores
internal_ids = [1234, 5678, 9012, 3456, 7890]

# Output
results = [
    SearchResult(
        doc=Doc(id='doc_abc', text='...', title='...', court='STF', ...),
        score=0.89
    ),
    # ... mais 4 resultados
]
```

---

#### 3.6 Resposta da Busca
**Formato JSON retornado para Interface:**

```json
{
  "query": "Quais sÃ£o os direitos fundamentais?",
  "total": 5,
  "backend": "faiss",
  "results": [
    {
      "id": "cf88_art5_inciso1",
      "title": "ConstituiÃ§Ã£o Federal - Artigo 5Âº",
      "text": "Todos sÃ£o iguais perante a lei, sem distinÃ§Ã£o de qualquer natureza...",
      "court": "STF",
      "code": "CF/88",
      "article": "5Âº, I",
      "date": "1988-10-05",
      "meta": {
        "url": "https://...",
        "type": "constitution"
      },
      "score": 0.8945
    },
    {
      "id": "cf88_art5_inciso2",
      "title": "ConstituiÃ§Ã£o Federal - Artigo 5Âº",
      "text": "NinguÃ©m serÃ¡ obrigado a fazer ou deixar de fazer alguma coisa...",
      "court": "STF",
      "code": "CF/88",
      "article": "5Âº, II",
      "date": "1988-10-05",
      "meta": {},
      "score": 0.8523
    }
    // ... mais 3 documentos
  ]
}
```

---

### **Fase 4: ConstruÃ§Ã£o do Prompt Augmented**

#### 4.1 FormataÃ§Ã£o do Contexto (Frontend)
**Arquivo:** `/Interface/app/pages/index.vue`

```typescript
if (dbvectorResponse.results && dbvectorResponse.results.length > 0) {
  // Formata os documentos encontrados como contexto
  const context = dbvectorResponse.results.map((result, index) => {
    const metadata = []
    if (result.court) metadata.push(`Tribunal: ${result.court}`)
    if (result.code) metadata.push(`CÃ³digo: ${result.code}`)
    if (result.article) metadata.push(`Artigo: ${result.article}`)
    if (result.date) metadata.push(`Data: ${result.date}`)
    
    return `
[Documento ${index + 1}${result.title ? ` - ${result.title}` : ''}]
${metadata.length > 0 ? metadata.join(' | ') : ''}
RelevÃ¢ncia: ${(result.score * 100).toFixed(1)}%

${result.text}
    `.trim()
  }).join('\n\n---\n\n')

  contextInfo = `ğŸ“š Consultados ${dbvectorResponse.results.length} documentos jurÃ­dicos (${dbvectorResponse.backend})`

  // Monta o prompt com contexto RAG
  finalPrompt = `VocÃª Ã© um assistente jurÃ­dico especializado. Use os seguintes documentos jurÃ­dicos como base para responder a pergunta do usuÃ¡rio de forma precisa e fundamentada.

DOCUMENTOS DE REFERÃŠNCIA:
${context}

PERGUNTA DO USUÃRIO:
${prompt}

INSTRUÃ‡Ã•ES:
- Baseie sua resposta nos documentos fornecidos
- Cite os documentos relevantes quando aplicÃ¡vel
- Se os documentos nÃ£o contiverem informaÃ§Ã£o suficiente, mencione isso
- Seja claro, objetivo e mantenha terminologia jurÃ­dica apropriada`
}
```

**LÃ³gica de ConstruÃ§Ã£o:**

1. **IteraÃ§Ã£o sobre Resultados:**
   - Para cada documento retornado pelo DBVECTOR
   - Extrai metadados estruturados (court, code, article, date)

2. **FormataÃ§Ã£o Individual:**
   - CabeÃ§alho: `[Documento N - TÃ­tulo]`
   - Metadados: `Tribunal: STF | CÃ³digo: CF/88 | Artigo: 5Âº`
   - RelevÃ¢ncia: `RelevÃ¢ncia: 89.5%` (score * 100)
   - ConteÃºdo: texto completo do documento

3. **SeparaÃ§Ã£o:**
   - Documentos separados por `\n\n---\n\n`
   - Facilita leitura pelo LLM

4. **Template do Prompt:**
   - **System Context:** Define papel (assistente jurÃ­dico)
   - **Documentos de ReferÃªncia:** Contexto recuperado
   - **Pergunta do UsuÃ¡rio:** Query original
   - **InstruÃ§Ãµes:** Diretrizes para o LLM

**Exemplo de Prompt ConstruÃ­do:**

```text
VocÃª Ã© um assistente jurÃ­dico especializado. Use os seguintes documentos jurÃ­dicos como base para responder a pergunta do usuÃ¡rio de forma precisa e fundamentada.

DOCUMENTOS DE REFERÃŠNCIA:
[Documento 1 - ConstituiÃ§Ã£o Federal - Artigo 5Âº]
Tribunal: STF | CÃ³digo: CF/88 | Artigo: 5Âº, I
RelevÃ¢ncia: 89.5%

Todos sÃ£o iguais perante a lei, sem distinÃ§Ã£o de qualquer natureza, garantindo-se aos brasileiros e aos estrangeiros residentes no PaÃ­s a inviolabilidade do direito Ã  vida, Ã  liberdade, Ã  igualdade, Ã  seguranÃ§a e Ã  propriedade...

---

[Documento 2 - ConstituiÃ§Ã£o Federal - Artigo 5Âº]
Tribunal: STF | CÃ³digo: CF/88 | Artigo: 5Âº, II
RelevÃ¢ncia: 85.2%

NinguÃ©m serÃ¡ obrigado a fazer ou deixar de fazer alguma coisa senÃ£o em virtude de lei...

---

[... 3 documentos adicionais ...]

PERGUNTA DO USUÃRIO:
Quais sÃ£o os direitos fundamentais previstos na ConstituiÃ§Ã£o?

INSTRUÃ‡Ã•ES:
- Baseie sua resposta nos documentos fornecidos
- Cite os documentos relevantes quando aplicÃ¡vel
- Se os documentos nÃ£o contiverem informaÃ§Ã£o suficiente, mencione isso
- Seja claro, objetivo e mantenha terminologia jurÃ­dica apropriada
```

---

### **Fase 5: GeraÃ§Ã£o da Resposta (LLM)**

#### 5.1 RequisiÃ§Ã£o para OpenAI (Frontend â†’ Backend)
**Arquivo:** `/Interface/app/pages/index.vue`

```typescript
const result = await $fetch<OpenAIResponse>('/api/openai/chat', {
  method: 'POST',
  body: {
    prompt: finalPrompt,
    model: model.value?.replace('openai/', '') || 'gpt-4o-mini'
  }
})
```

**ParÃ¢metros:**
- `prompt`: Prompt completo (query + contexto RAG)
- `model`: Modelo OpenAI selecionado pelo usuÃ¡rio
  - `gpt-4o-mini` (default, rÃ¡pido, econÃ´mico)
  - `gpt-4o` (mais capaz, mais caro)
  - `gpt-5-mini` / `o1-mini` (reasoning models)

---

#### 5.2 API OpenAI Wrapper (Backend Nuxt)
**Arquivo:** `/Interface/server/api/openai/chat.post.ts`

```typescript
export default defineEventHandler(async (event) => {
  const body = await readBody(event)
  const config = useRuntimeConfig()
  
  const modelName = body.model || 'gpt-4o-mini'
  const isReasoningModel = modelName.includes('o1') || modelName.includes('gpt-5')
  
  const requestBody: any = {
    model: modelName,
    messages: [
      {
        role: 'user',
        content: body.prompt
      }
    ],
    max_completion_tokens: 10000
  }

  // Modelos de reasoning nÃ£o suportam temperature
  if (!isReasoningModel) {
    requestBody.temperature = 1
  }

  const response = await $fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${config.openaiApiKey}`,
      'Content-Type': 'application/json',
      ...(config.openaiProjectId ? { 'OpenAI-Project': config.openaiProjectId } : {})
    },
    body: requestBody
  })

  return response
})
```

**LÃ³gica TÃ©cnica:**

1. **SeguranÃ§a:**
   - API key armazenada no servidor (variÃ¡vel ambiente)
   - Nunca exposta ao frontend
   - Backend atua como proxy seguro

2. **ConfiguraÃ§Ã£o do Request:**
   - **model:** Modelo LLM especÃ­fico
   - **messages:** Array com role + content
   - **max_completion_tokens:** Limite de tokens na resposta (10k)
   - **temperature:** Criatividade (0-2, default=1)
     - Reasoning models (o1, gpt-5) nÃ£o usam temperature

3. **Headers:**
   - `Authorization`: Bearer token com API key
   - `OpenAI-Project`: ID do projeto (opcional)

4. **Endpoint:**
   - `https://api.openai.com/v1/chat/completions`
   - API oficial OpenAI Chat Completions

---

#### 5.3 Processamento pelo LLM (OpenAI)

**Processo Interno do GPT:**

1. **TokenizaÃ§Ã£o:**
   - Prompt Ã© dividido em tokens (subwords)
   - Modelo: ~750 tokens por 1000 caracteres
   - Limite contextual: 128k tokens (gpt-4o-mini)

2. **Encoding:**
   - Tokens â†’ embeddings (dimensÃ£o 12288 para GPT-4)
   - Positional encoding adicionado

3. **Transformer Layers:**
   - Self-attention multi-head
   - Feed-forward networks
   - ~50 layers (GPT-4o-mini)

4. **Generation:**
   - Autoregressive: gera token por token
   - Sampling com temperature
   - Top-p (nucleus sampling) para diversidade

5. **Grounding:**
   - LLM "lÃª" os documentos fornecidos no contexto
   - Usa informaÃ§Ãµes factuais para fundamentar resposta
   - Reduz alucinaÃ§Ãµes (fabricaÃ§Ã£o de informaÃ§Ãµes)

**Exemplo de Response:**

```json
{
  "id": "chatcmpl-AbC123",
  "object": "chat.completion",
  "created": 1733688000,
  "model": "gpt-4o-mini-2024-07-18",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Com base nos documentos jurÃ­dicos consultados, os direitos fundamentais previstos na ConstituiÃ§Ã£o Federal de 1988 incluem:\n\n1. **Igualdade** (Art. 5Âº, I): Todos sÃ£o iguais perante a lei...\n\n2. **Legalidade** (Art. 5Âº, II): NinguÃ©m serÃ¡ obrigado a fazer ou deixar de fazer...\n\n[... resposta completa fundamentada nos documentos ...]"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 2543,
    "completion_tokens": 487,
    "total_tokens": 3030
  }
}
```

---

### **Fase 6: ApresentaÃ§Ã£o da Resposta**

#### 6.1 ExtraÃ§Ã£o e FormataÃ§Ã£o (Frontend)
**Arquivo:** `/Interface/app/pages/index.vue`

```typescript
// Extrai o texto da resposta
const extractedText = result?.choices?.[0]?.message?.content || ''

// Adiciona informaÃ§Ã£o de contexto
const finalResponse = contextInfo 
  ? `${contextInfo}\n\n${extractedText}`
  : extractedText

response.value = finalResponse
```

**LÃ³gica:**

1. **ExtraÃ§Ã£o:**
   - Navega na estrutura JSON: `choices[0].message.content`
   - Fallback para string vazia se nÃ£o existir

2. **Enriquecimento:**
   - Adiciona badge informativo: `ğŸ“š Consultados 5 documentos jurÃ­dicos (faiss)`
   - Separa badge da resposta com `\n\n`

3. **RenderizaÃ§Ã£o:**
   - Resposta Ã© exibida na interface
   - Suporte para Markdown (listas, negrito, etc.)
   - BotÃ£o de copiar para clipboard

**Exemplo de Resposta Final:**

```
ğŸ“š Consultados 5 documentos jurÃ­dicos (faiss)

Com base nos documentos jurÃ­dicos consultados, os direitos fundamentais previstos na ConstituiÃ§Ã£o Federal de 1988 incluem:

1. **Igualdade** (Art. 5Âº, I): Todos sÃ£o iguais perante a lei, sem distinÃ§Ã£o de qualquer natureza, garantindo-se aos brasileiros e aos estrangeiros residentes no PaÃ­s a inviolabilidade do direito Ã  vida, Ã  liberdade, Ã  igualdade, Ã  seguranÃ§a e Ã  propriedade.

2. **Legalidade** (Art. 5Âº, II): NinguÃ©m serÃ¡ obrigado a fazer ou deixar de fazer alguma coisa senÃ£o em virtude de lei.

[... resposta completa ...]

Esses direitos estÃ£o fundamentados nos documentos consultados da base de conhecimento jurÃ­dica.
```

---

## ğŸ” ComparaÃ§Ã£o: RAG vs. Chat Simples

### **Modo RAG (useRAG = true)**

```
UsuÃ¡rio â†’ Interface â†’ DBVECTOR â†’ Interface â†’ OpenAI â†’ Interface â†’ Resposta Fundamentada
          â†“           â†“                       â†“
      Busca Docs   Retrieval             Prompt + Context
```

**CaracterÃ­sticas:**
- âœ… Resposta baseada em documentos reais
- âœ… Reduz alucinaÃ§Ãµes
- âœ… Cita fontes jurÃ­dicas
- âœ… Maior latÃªncia (~2-4s)
- âœ… Mais preciso e confiÃ¡vel

### **Chat Simples (useRAG = false)**

```
UsuÃ¡rio â†’ Interface â†’ OpenAI â†’ Interface â†’ Resposta GenÃ©rica
                      â†“
                  Apenas LLM
```

**CaracterÃ­sticas:**
- âœ… Menor latÃªncia (~1-2s)
- âŒ NÃ£o consulta base de conhecimento
- âŒ Pode alucinar informaÃ§Ãµes
- âŒ Sem fontes jurÃ­dicas
- âœ… Bom para conversas gerais

---

## ğŸ“Š MÃ©tricas e Performance

### **LatÃªncias TÃ­picas**

| Fase | OperaÃ§Ã£o | Tempo MÃ©dio |
|------|----------|-------------|
| 1 | GeraÃ§Ã£o Embedding Query | 50-100ms |
| 2 | Busca FAISS (724k docs) | 100-200ms |
| 3 | ConstruÃ§Ã£o Prompt | 10-20ms |
| 4 | Chamada OpenAI | 1500-3000ms |
| **Total** | **Modo RAG** | **~2-4s** |
| **Total** | **Chat Simples** | **~1-2s** |

### **Recursos Computacionais**

**DBVECTOR:**
- CPU: 2-4 cores (para embedding + FAISS)
- RAM: ~4GB (Ã­ndice + metadados + modelo)
- GPU (opcional): Acelera embedding e FAISS

**Interface:**
- Leve: apenas proxy HTTP
- RAM: ~200MB

**OpenAI:**
- Serverless (API externa)
- Pay-per-use

---

## ğŸ” ConfiguraÃ§Ãµes Importantes

### **Environment Variables**

**DBVECTOR (`.env`):**
```bash
# Backend de busca
SEARCH_BACKEND=faiss  # ou 'opensearch'

# Modelo de embedding
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIM=384
NORMALIZE_EMBEDDINGS=true

# FAISS
FAISS_INDEX_PATH=data/indexes/faiss
FAISS_METADATA_PATH=data/indexes/faiss/metadata.parquet
USE_FAISS_GPU=false  # true se GPU disponÃ­vel
FAISS_GPU_DEVICE=0

# API
API_HOST=0.0.0.0
API_PORT=8000
```

**Interface (`.env`):**
```bash
# OpenAI
OPENAI_API_KEY=sk-proj-...
OPENAI_PROJECT_ID=proj_...  # opcional

# DBVECTOR
NUXT_PUBLIC_DBVECTOR_API_URL=http://localhost:8000
```

---

## ğŸ§ª Testando o Fluxo

### **1. Verificar DBVECTOR**
```bash
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"q": "direitos fundamentais", "k": 3}'
```

**Resposta Esperada:**
```json
{
  "query": "direitos fundamentais",
  "total": 3,
  "backend": "faiss",
  "results": [...]
}
```

### **2. Testar Interface RAG**
1. Acesse `http://localhost:3000`
2. Selecione modo "RAG"
3. Digite: "Quais sÃ£o os direitos fundamentais?"
4. Observe logs no console (F12)

**Logs Esperados:**
```
[HomePage] Creating new chat { promptLength: 40, useRAG: true }
[HomePage] RAG mode enabled - searching DBVECTOR { query: "..." }
[DBVECTOR API] Sending request to DBVECTOR { url: "...", k: 5 }
[DBVECTOR API] DBVECTOR search successful { total: 5, backend: "faiss" }
[HomePage] DBVECTOR search completed { total: 5, resultsCount: 5 }
[OpenAI API] Sending request to OpenAI { model: "gpt-4o-mini", ... }
[OpenAI API] OpenAI request successful { contentLength: 1234 }
[HomePage] Request completed { responseLength: 1300 }
```

---

## ğŸ› Troubleshooting

### **Problema: "Store nÃ£o inicializado"**
**Causa:** DBVECTOR nÃ£o carregou Ã­ndice FAISS  
**SoluÃ§Ã£o:**
```bash
cd DBVECTOR
make faiss-build
```

### **Problema: "Nenhum documento indexado"**
**Causa:** Ãndice FAISS vazio  
**SoluÃ§Ã£o:**
```bash
# Verificar dados
ls data/merged_clean.jsonl

# Rebuild index
python -m src.pipelines.build_faiss
```

### **Problema: Resposta nÃ£o fundamentada**
**Causa:** Modo RAG desativado ou erro no DBVECTOR  
**SoluÃ§Ã£o:**
1. Verificar toggle na interface (deve estar em "RAG")
2. Verificar logs: deve ter "RAG mode enabled"
3. Verificar saÃºde do DBVECTOR: `curl http://localhost:8000/health`

### **Problema: LatÃªncia muito alta (>10s)**
**Causa:** Modelo embedding lento ou GPU nÃ£o configurada  
**SoluÃ§Ã£o:**
1. Considerar modelo menor: `all-MiniLM-L6-v2` (384d) vs `all-mpnet-base-v2` (768d)
2. Habilitar GPU: `USE_FAISS_GPU=true` (se disponÃ­vel)
3. Reduzir `k` (menos documentos recuperados)

---

## ğŸ“š ReferÃªncias TÃ©cnicas

### **Bibliotecas Principais**

- **sentence-transformers**: GeraÃ§Ã£o de embeddings
- **FAISS**: Busca vetorial eficiente (Facebook AI)
- **FastAPI**: API backend Python
- **Nuxt 3**: Framework frontend/backend
- **OpenAI API**: LLM (GPT-4o-mini)

### **Papers e Conceitos**

- **RAG:** Lewis et al., 2020 - "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
- **FAISS:** Johnson et al., 2019 - "Billion-scale similarity search with GPUs"
- **Sentence-BERT:** Reimers & Gurevych, 2019 - "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"

---

## ğŸ¯ Melhorias Futuras

### **Performance**
- [ ] Cache de embeddings de queries frequentes
- [ ] PrÃ©-ranking com BM25 + re-ranking com FAISS
- [ ] QuantizaÃ§Ã£o de embeddings (int8) para reduzir memÃ³ria

### **Qualidade**
- [ ] Reranker cross-encoder para refinar top-k
- [ ] Feedback de relevÃ¢ncia (usuÃ¡rio marca respostas Ãºteis)
- [ ] Chunking inteligente de documentos longos

### **Infraestrutura**
- [ ] Deploy com Docker Compose
- [ ] Monitoramento com Prometheus + Grafana
- [ ] Rate limiting e autenticaÃ§Ã£o
- [ ] Backup incremental do Ã­ndice FAISS

---

## ğŸ“ ConclusÃ£o

O fluxo RAG implementado neste sistema combina:

1. **Busca Vetorial Eficiente:** FAISS para recuperar documentos relevantes em milissegundos
2. **Embeddings SemÃ¢nticos:** Sentence-transformers para capturar significado, nÃ£o apenas palavras-chave
3. **Augmented Generation:** LLM fundamentado em contexto jurÃ­dico real
4. **Arquitetura Modular:** Componentes independentes e escalÃ¡veis

**Resultado:** Respostas jurÃ­dicas precisas, fundamentadas e verificÃ¡veis, reduzindo drasticamente alucinaÃ§Ãµes do LLM e aumentando confiabilidade do sistema.
